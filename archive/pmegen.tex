A proof that \textsc{pme} generalizes standard conditioning is in
\scite{7}{williams80}{}. A proof that \textsc{pme} generalizes Jeffrey
conditioning is in \scite{7}{jeffrey65}{}. I will give my own simple
proofs here that are more in keeping with the notation in the paper.
An interested reader can also apply these proofs to show that
\textsc{pme} generalizes Wagner conditioning, but not without
simplifications that compromise mathematical rigour. The more rigorous
proof for the generalization of Wagner conditioning is in the body of
the paper.

I assume finite (and therefore discrete) probability distributions.
For countable and continuous probability distributions, the reasoning
is largely analogous (for an introduction to continuous entropy see
\scite{8}{guiasu77}{16ff}; for an example of how to do a proof of this
section for continuous probability densities see
\scite{7}{catichagiffin06}{}, and \scite{7}{jaynes78}{}; for a proof
that the stationary points of the Lagrange function are indeed the
desired extrema see \scite{8}{zubarevetal74}{55}, and
\scite{8}{coverthomas06}{410}; for the pioneer of the method applied
in this section see \scite{8}{jaynes78}{241ff}).

\section{Standard Conditioning}
\label{sc}

Let $y_{i}$ (all $y_{i}\neq{}0$) be a finite type II prior probability
distribution summing to $1$, $i\in{}I$. Let $\hat{y}_{i}$ be the
posterior probability distribution derived from standard conditioning
with $\hat{y}_{i}=0$ for all $i\in{}I'$ and $\hat{y}_{i}\neq{}0$ for
all $i\in{}I''$, $I'\cup{}I''=I$. $I'$ and $I''$ specify the standard
event observation. Standard conditioning requires that

\begin{equation}
  \label{eq:sc}
  \hat{y}_{i}=\frac{y_{i}}{\sum_{k\in{}I''}y_{k}}.
\end{equation}

{\noindent}To solve this problem using \textsc{pme}, we want to minimize the
cross-entropy with the constraint that the non-zero $\hat{y}_{i}$ sum to
$1$. The Lagrange function is (writing in vector form
$\hat{y}=(\hat{y}_{i})_{i\in{}I''}$)

\begin{equation}
  \label{eq:sclag}
  \Lambda(\hat{y},\lambda)=\sum_{i\in{}I''}\hat{y}_{i}\ln\frac{\hat{y}_{i}}{y_{i}}+\lambda\left(1-\sum_{i\in{}I''}\hat{y}_{i}\right).
\end{equation}

{\noindent}Differentiating the Lagrange function with respect to $\hat{y}_{i}$ and
setting the result to zero gives us

\begin{equation}
  \label{eq:sc1}
  \hat{y}_{i}=y_{i}e^{\lambda-1}
\end{equation}

{\noindent}with $\lambda$ normalized to

\begin{equation}
  \label{eq:sc2}
  \lambda=-1+\ln{}\sum_{i\in{}I''}y_{i}.
\end{equation}

{\noindent}(\ref{eq:sc}) follows immediately. \textsc{pme} generalizes standard conditioning.

\section{Jeffrey Conditioning}
\label{jco}

Let $\theta_{i},i=1,\ldots,n$ and $\omega_{j},j=1,\ldots,m$ be finite
partitions of the event space with the joint prior probability matrix
$(y_{ij})$ (all $y_{ij}\neq{}0$). Let $\kappa$ be defined as in
Section \ref{jc}, with (\ref{eq:m1}) true (remember that in Section
\ref{Generalization}, (\ref{eq:m1}) is no longer required). Let $P$ be
the type II prior probability distribution and $\hat{P}$ the posterior
probability distribution.

Let $\hat{y}_{ij}$ be the posterior probability distribution derived
from Jeffrey conditioning with

\begin{equation}
  \label{eq:jc1}
  \sum_{i=1}^{n}\hat{y}_{ij}=\hat{P}(\omega_{j})\mbox{ for all }j=1,\ldots,m
\end{equation}

{\noindent}Jeffrey conditioning requires that for all $i=1,\ldots,n$

\begin{equation}
  \label{eq:jc2}
  \hat{P}(\theta_{i})=\sum_{j=1}^{m}P(\theta_{i}|\omega_{j})\hat{P}(\omega_{j})=\sum_{j=1}^{m}\frac{y_{ij}}{P(\omega_{j})}\hat{P}(\omega_{j})
\end{equation}

{\noindent}Using \textsc{pme} to get the posterior distribution
$(\hat{y}_{ij})$, the Lagrange function is (writing in vector form
$\hat{y}=(x_{11},\ldots,x_{n1},\ldots,x_{nm})^{\top}$ and
$\lambda=(\lambda_{1},\ldots,\lambda_{m})^{\top}$)

\begin{equation}
  \label{eq:jclag}
  \Lambda(\hat{y},\lambda)=\sum_{i=1}^{n}\sum_{j=1}^{m}\hat{y}_{ij}\ln\frac{\hat{y}_{ij}}{y_{ij}}+\sum_{j=1}^{m}\lambda_{j}\left(\hat{P}(\omega_{j})-\sum_{i=1}^{n}\hat{y}_{ij}\right).
\end{equation}

{\noindent}Consequently,

\begin{equation}
  \label{eq:jc4}
  \hat{y}_{ij}=y_{ij}e^{\lambda_{j}-1}
\end{equation}

{\noindent}with the Lagrangian parameters $\lambda_{j}$ normalized by

\begin{equation}
  \label{eq:jc5}
  \sum_{i=1}^{n}y_{ij}e^{\lambda_{j}-1}=\hat{P}(\omega_{j})
\end{equation}

{\noindent}(\ref{eq:jc2}) follows immediately. \textsc{pme}
generalizes Jeffrey conditioning.