In the early 1970s, the dominant models for similarity in the
psychological literature were all geometric in nature. Distance
measures capturing similarity and dissimilarity between concepts
obeyed minimality, symmetry, and the triangle inequality. Then Amos
Tversky wrote a compelling paper, "Features of Similarity,"
undermining the idea that a metric topology is the best model. Tversky
gave both theoretical and empirical reasons why similarity between
concepts did not fulfill minimality, symmetry, or the triangle
inequality. Geometry with its metric distance measures was not a
helpful way to model similarity. Tversky presented an alternative
set-theoretic approach which accommodated the data that could not be
reconciled with a geometry of similarity.

The aim of this paper is to help along a similar paradigm shift when
it comes to epistemic modeling of closeness or difference between
subjective probability distributions. The geometry of reason (a term
coined by Richard Pettigrew and Hannes Leitgeb, two of its advocates)
also violates reasonable expectation we may have toward an acceptable
model. Just as Tversky did, I will present a non-geometric and
asymmetric alternative: information theory. Information theory
fulfills the expectations that the geometry of reason violates and
incorporates basic Bayesian commitments to probabilism and standard
conditioning.

The epistemic utility approach in Bayesian epistemology has attracted
some attention in the past few years. James Joyce, in an article
programmatically named A Nonpragmatic Vindication of Probabilism,
defends probabilism supported by partial-belief-based epistemic
utility rather than the pragmatic utility common in Dutch-book style
arguments. For Joyce, norms of gradational accuracy characterize the
epistemic utility approach to partial beliefs, analogous to norms of
truth for full beliefs.

Richard Pettigrew and Hannes Leitgeb have published arguments that
under certain assumptions probabilism and standard conditioning (which
together give epistemology a distinct Bayesian flavour) minimize
inaccuracy, thereby providing maximal epistemic utility. Leitgeb and
Pettigrew show, given the geometry of reason and other axioms inspired
by Joyce (for example normality and dominance), that in order to avoid
epistemic dilemmas we must commit ourselves to a Brier score measure
of inaccuracy and subsequently to probabilism and standard
conditioning. Jeffrey conditioning (also called probability
kinematics) is widely considered to be a commonsense extension of
standard conditioning. On Leitgeb and Pettigrew's account, it fails to
provide maximal epistemic utility. Another type of conditioning, which
we will call LP conditioning, takes the place of Jeffrey conditioning.

The failure of Jeffrey conditioning to minimize inaccuracy on the
basis of the geometry of reason casts, by reductio, doubt on the
geometry of reason. I will show that LP conditioning, which the
geometry of reason entails, fails seven commonsense expectations that
are reasonable to have for the kind of updating scenario that LP
conditioning addresses. Leitgeb and Pettigrew do little to
substantiate a link between the geometry of reason and epistemic
utility on a conceptual level. It is the formal success of the model
that makes the geometry of reason attractive, but the failure of LP
conditioning to meet basic expectations undermines this success.

The question then remains whether we have a plausible candidate to
supplant the geometry of reason. The answer is yes: information theory
provides us with a measure of closeness between probability
distributions on a finite event space that has more conceptual appeal
than the geometry of reason, especially with respect to epistemic
utility---it is intuitively correct to relate coming-to-knowledge to
exchange of information. More persuasive than intuitions, however, is
the fact that information theory supports both standard conditioning
and the extension of standard conditioning to Jeffrey conditioning, an
extension which is formally continuous with the standard conditioning
which Leitgeb and Pettigrew have worked so hard to vindicate
nonpragmatically. LP conditioning is not continuous with standard
conditioning---one of the seven expectations that LP conditioning
fails to meet.

Leitgeb and Pettigrew's reasoning to establish LP conditioning on the
basis of the geometry of reason is valid. Given the failure of LP
conditioning with respect to the seven expectations, it cannot be
sound. The premise to reject is the geometry of reason. Fortunately,
information theory replaces it and yields results that fulfill the
seven expectations, although I also note in the paper that the burden
is on information theory to give an epistemic justification explaining
the non-trivial structure of its symmetry breaking.
