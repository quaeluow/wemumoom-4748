%\documentclass[10pt]{article}
\documentclass[landscape,10pt]{article}

\setlength{\parindent}{0in}
\setlength{\parskip}{.3in}

\raggedbottom

\pagestyle{empty}

% 	PACKAGES
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[fleqn]{amsmath}
%\usepackage{german}
%\usepackage{hebtex}
%\usepackage{pdfpages} % to include pdf pages
%\usepackage{graphics}
% \usepackage[german]{babel}
%\usepackage{helvet}

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\PP}{\mathbb{P}}

\begin{document}

Naohito,

Your 1993 paper, Geometrical Structures of Some Non-Distance Models
for Asymmetric MDS, is very helpful. I was wondering about
transitivity. If $d$ is an asymmetric distance measure and

\begin{equation}
  \label{eq:e1}
  \Delta(p,q)=d(q,p)-d(p,q)
\end{equation}

then one would usually expect that $\Delta(P,Q)>0$ and $\Delta(Q,R)>0$
implies $\Delta(P,R)>0$. That is if the asymmetry is well-behaved.

I am particularly interested in the Kullback-Leibler divergence:

\begin{equation}
  \label{eq:e2}
  D_{\mbox{\tiny KL}}(P,Q)=\sum_{i=1}^{n}p_{i}\log\frac{p_{i}}{q_{i}}
\end{equation}

where $P,Q$ are finite probability distributions with

\begin{equation}
  \label{eq:e3}
  \sum_{i=1}^{n}p_{i}=1.
\end{equation}

It turns out that the KL divergence is not well-behaved. For example, if

\begin{eqnarray}
\label{eq:e4}
  P&=&(1/3,1/3,1/3)\\
  Q&=&(1/2,1/4,1/4)\\
  R&=&(2/5,2/5,1/5)
\end{eqnarray}

\begin{eqnarray}
\label{eq:e5}
D(P,Q)&>&0\\
D(Q,R)&>&0\\
D(P,R)&<&0
\end{eqnarray}

You made the following suggestion in your email:

\begin{quote}
  Very interesting! I suppose that your finding may be explained by
  examining the definiteness of the Hermitian matrix
  $H=(S+S')/2+i*(S-S')/2$ constructed from the proximity matrix $S$
  which you specified above. Here, $S'$ denotes the transposed matrix
  of $S$, and $i$ denotes the pure imaginary number. This inspection
  comes from the theory of the Hermitian Form Model (HFM) proposed by
  Chino and Shiraiwa (1993), Behaviormetrika. Our theory states that
  members (or objects) are embedded in a finite-dimensional Hilbert
  space if and only if the matrix $H$ is positive-semi definite.
\end{quote}

I would love to show that this is the case and your hunch is right.
Transitivity is violated if for $S'$ (your skew-symmetric matrix)

\begin{equation}
  \label{eq:e6}
      \left[
      \begin{array}{ccc}
        0 & k_{12} & k_{13} \\
        k_{21} & 0 & k_{23} \\
        k_{31} & k_{32} & 0 
      \end{array}
\right].
\end{equation}

$k_{12}$ and $k_{23}$ agree in their sign against $k_{13}$ (for
example: $k_{12}>0,k_{23}>0,k_{13}<0$). 

Take, for example the following proximity data matrix

\begin{equation}
  \label{eq:e7}
      \left[
      \begin{array}{ccc}
        0 & 2 & 3 \\
        3 & 0 & 1 \\
        -1 & 2 & 0 
      \end{array}
\right].
\end{equation}

Let $H$ be defined as in your paper on page 36. Then

\begin{equation}
  \label{eq:e8}
  \det(H-\lambda{}I)=-\lambda^{3}+14\lambda-1
\end{equation}

There are then three real-valued eigenvalues
$\lambda_{1},\lambda_{2},\lambda_{3}$. What you call $\Lambda$ and
$U_{1}$ on page 37 looks as follows:

\begin{equation}
  \label{eq:e72}
\Lambda=
      \left[
      \begin{array}{ccc}
  -3.78 & 0 & 0 \\
  0 & 0.0715 & 0 \\
  0 & 0 & 3.71
      \end{array}
\right],
\end{equation}

\begin{equation}
  \label{eq:e71}
U_{1}=
      \left[
      \begin{array}{ccc}
   0.0185 + 0.639i & -0.375 + 0.199i &  0.514 + 0.386i \\
   0.279 - 0.494i & -0.169 - 0.573i &  0.503 + 0.260i \\
  -0.519 & 0.681 & 0.516
      \end{array}
\right].
\end{equation}

All of this makes good sense, but then I get confused on page 38. In
your 1978 paper, the idea was to map the objects whose distance are
measured into some kind of space in order to model them (first, you
did this for two-dimensional spaces, then for three, and then you
generalized it to higher-dimensional spaces in later papers). The size
of the parallelogram between them was indicative of asymmetries (cross
product), whereas the inner product was indicative of their mutual
proximity or distance. The orientation indicated which way the
asymmetry went.

I take the 1993 paper to propose a non-spatial model where the objects
are mapped onto an f.d.c.\ Hilbert space. Cross product and inner
product are still indicative of the skew-symmetry and symmetry. The
inner product, which chracterizes the Hilbert space, is

\begin{equation}
  \label{eq:e9}
  \phi(\zeta,\tau)=\zeta\Lambda\tau^{*}.
\end{equation}

What I don't see explained is the following question:

Where are the original objects in the Hilbert space? In my proximity
data matrix (\ref{eq:e7}), for example, the distance measured from
object 1 to object 2 is $2$ and the distance measured from object 2 to
object 1 is $3$. Where are these objects in the f.d.c. Hilbert space
characterized by (\ref{eq:e9})? Do we have to go through the long and
painful iteration process of your 1978 paper? I thought for a moment
that your $x_{i}$ in your formula (13) on page 38 may have something
to do with this, but their dimension is double of what we have in the
Hilbert space. In any case, why did you need
$X,\Omega_{s},\Omega_{sk}$ on page 38? Once we have the objects mapped
onto the Hilbert space, we have the inner product defined by
(\ref{eq:e9}), but how do we define the cross-product? It is usually
only defined for three dimensional space. All of this is not clear to
me.

\end{document}