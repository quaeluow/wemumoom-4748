Paul,

The conference in New Brunswick was great. There were a lot of good talks, both at the CSHPS and at the CPA meeting. My own talk went very well, there were some good questions afterwards (one person brought up the Monty Hall problem the same way you brought up the Sleeping Beauty problem and wondered if maximum entropy could be applied there). I am attaching my slideshow (use fullscreen in your pdf viewer). The presentation changed quite a bit because a couple of days after I talked to you I caught a mistake in what I had told you. On the handout I gave you I was asking this question at the end, how applying MAXENT to (MAP) first and then to (HQ) could give us a different posterior distribution than applying it to (HQ) first and then to (MAP), when MINXENT is (basically) MAXENT plus Dempster's Rule of Combination, which is symmetrical. It turned out that this suspicion was justified and asymmetry is not the problem. In the meantime, however, I've found a few other trails that are worth pursuing. The slides Intuition (T1) and Intuition (T2) capture the tension between Halpern's approach and Jaynes' approach, and I think there is a lot of potential for criticizing Halpern. The question is: does an increase in knowledge about one partition of the event space (based on partial information about expectations and conditional probabilities) increase the probability of the other partition about which we have no such knowledge (blue, in Judy Benjamin's case)? Should P(B) be .5 or .53? Halpern says it should be .5 based on some uniformity assumptions. Jaynes says it should be .53 based on MAXENT. I have a suspicion that using more and more finely grained power sets (a much weaker assumption than Halpern's!) supports Jaynes' version, but the math is at this point intractable to me. I also think that Halpern is making assumptions about independence (that's where Dempster's Rule of Combination comes in again) that presume information we don't have.

So, I am still having fun with this and would like to spend the summer on trying to work it into a paper (if the results support it, which in these cases is often hard to predict -- I've fallen flat on my face under similar circumstances several times now). I am gaining a much more differentiated picture about the relationship between Bayesian conditionalization and MAXENT, which invalidates some sweeping generalizations I used to make (in my SSHRC application, for example), but also opens up questions that I find very intriguing. So, if all goes well, at the end of the summer I'd love to send you something more mature than what I've given you so far (wouldn't I wish) and get your feedback. I think I am planning on taking two seminars in the fall and one in the spring, but I will begin work on my comps papers as well (Alan liked my Carnap paper reasonably well, which I could pursue; I'd like to take another stab at the Continuous Prisoner's Dilemma with Chris Stephens; the MAXENT paper is a third possibility, perhaps Adam Morton would be willing to take it on).

Thank you for your input so far!

Stefan
