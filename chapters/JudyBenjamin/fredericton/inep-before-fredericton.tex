\documentclass[11pt]{article}

\setlength{\marginparwidth}{1.2in}
\let\oldmarginpar\marginpar
\renewcommand\marginpar[1]{\-\oldmarginpar[\raggedleft\footnotesize #1]%
{\raggedright\footnotesize #1}}

\frenchspacing % no extra space at the end of a sentence

\setlength{\parindent}{0in}
\setlength{\parskip}{.1in}

\raggedbottom

%\pagestyle{empty}

% 	PACKAGES
% \usepackage[small,bf]{caption}
% \let\bcode\textbgreek
% \usepackage[bgreek,english]{babel}
% \usepackage{setspace}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
% \usepackage{german}
% \usepackage{hebtex}
% \usepackage{graphicx}
% \usepackage[german]{babel}
% \usepackage{endnotes}
% \let\footnote=\endnote
% \usepackage{rotating}
% \usepackage{enumerate}

\newcommand{\kapt}[1]{\textbf{{\thechap}. #1}\addtocounter{chap}{1}}

\newcommand{\tbd}[1]{}
\newcommand{\qnull}[1]{`#1'}
\newcommand{\qeins}[1]{``#1''}
\newcommand{\qzwei}[1]{`#1'}

\newif\ifNumericalOrYear
\NumericalOrYeartrue
% \NumericalOrYearfalse
\ifNumericalOrYear
\usepackage[numbers,colon]{natbib}
\else
\usepackage[round,colon]{natbib}
\fi
\newif\ifPageP
\PagePtrue
% \PagePfalse
\ifPageP
\newcommand{\PageP}{p.~}
\else
\newcommand{\PageP}{}
\fi

\newcommand{\scite}[3]{\ifnum#1=1\ifNumericalOrYear\citep{#2}\else\citeyearpar{#2}\fi\else
\ifnum#1=2\ifNumericalOrYear\citep[#3]{#2}\else\citep[{\PageP}#3]{#2}\fi\else
\ifnum#1=3\ifNumericalOrYear(\citet[#3]{#2})\else\citep[{\PageP}#3]{#2}\fi\else
\ifnum#1=4\ifNumericalOrYear\citet{#2}\else\citet{#2}\fi\else
\ifnum#1=5\ifNumericalOrYear(\citet{#2})\else\citep{#2}\fi\else
\ifnum#1=6\ifNumericalOrYear(\citet[#3]{#2})\else\citep[{\PageP}#3]{#2}\fi\else
\ifnum#1=7\ifNumericalOrYear\citep{#2}\else\citealp{#2}\fi\else
\ifnum#1=8\ifNumericalOrYear\citep[#3]{#2}\else\citealp[{\PageP}#3]{#2}\fi\else
\textbf{[invalid scite code]}\fi\fi\fi\fi\fi\fi\fi\fi}

\newenvironment{quotex}{\begin{quote}\begin{footnotesize}}{\end{footnotesize}\end{quote}}
% \newenvironment{quotex}{\begin{quote}\begin{footnotesize}\begin{singlespace}}{\end{singlespace}\end{footnotesize}\end{quote}}

\begin{document}

\title{Probability Kinematics and Halpern's Full Employment Theory}

\author{Stefan Lukits}

\maketitle

\newcounter{chap}

\setcounter{chap}{1}

\kapt{Introduction}

Describe Halpern's full employment theorem and argue that Halpern does
little to justify it with respect to \textsc{maxent} and \textsc{minxent}.

\kapt{Judy Benjamin}

There are two conflicting intuitions for the Judy Benjamin case. (I1)
When Judy Benjamin learns that $Q(A_{1}|A_{1}\cup{}A_{2})=.75$ (or
$q=.75$) it should not affect the probability she assigns to being in
blue territory, i.e.\ $Q(A_{3})=P(A_{3})$. (For now, our understanding
is that $P$ is the prior probability distribution and $Q$ is the
posterior probability distribution relative to the information from
her headquarters. We also abbreviate
$q_{1}=Q(A_{1}),q_{2}=Q(A_{2}),q_{3}=Q(A_{3})$.) (I2) If her
headquarters tell her that $q=1-\varepsilon$ with $\varepsilon$ a
small number, we would expect
$q_{1}\rightarrow{}0,q_{2}\rightarrow{}1/3,q_{3}\rightarrow{}2/3$. (I1)
and (I2) are contradictory if the map $G:q\rightarrow{}q_{1}$ is to be
continuous.

Giving us very little detail, Bas van Fraassen uses Dempster's Rule of
Combination to combine the prior probability distribution with the
principle of maximum entropy (\textsc{maxent}) to provide the
posterior probability distribution suggested by the principle of
minimal discrimination (sometimes called \textsc{minxent} in analogy
to \textsc{maxent}). van Fraassen does not mention that he is using
Dempster's Rule, nor does he mention that the Rule combined with
\textsc{maxent} provides us with the result required by
\textsc{minxent}. His assumptions are correct (see exercise 12.2 in
\scite{coverthomas06}{7}{}), however, and Judy Benjamin's posterior
probability distribution required by \textsc{minxent} is approximately
\begin{equation}
  \label{eq:minxent}
  v_{1}=(.12,.35,.53)
\end{equation}

($v_{1}$ is, in van Fraassen's terminology, the normalized odds vector
$(Q(A_{i}))_{i=1,\ldots,m}$. $v_{0}$ is the normalized odds vector for
$P$, i.e.\ $v_{0}=(.25,.25,.5)$.) To fill out van Fraassen's
description of the Judy Benjamin case and make further observations
about (I1) and (I2), we will provide in the next section a proof of
the constraint rule and then use the constraint rule to sort out our
intuitions.

\kapt{The Constraint Rule}

Let $f$ be a probability distribution on a finite space
$x_{1},\ldots,x_{m}$ that fulfills the constraint
\begin{equation}
  \label{eq:constraint}
\sum_{i=1}^{m}r(x_{i})f(x_{i})=\alpha
\end{equation}

Because $f$ is a probability distribution it fulfills
\begin{equation}
  \label{eq:unity}
\sum_{i=1}^{m}f(x_{i})=1
\end{equation}

We want to maximize the entropy, given the constraints
({\ref{eq:constraint}}) and ({\ref{eq:unity}}),
\begin{equation}
  \label{eq:entropy}
-\sum_{i=1}^{m}f(x_{i})\ln(x_{i})
\end{equation}

We use Lagrange multipliers to define the functional
\begin{equation}
  \label{eq:functional}
J(f)=-\sum_{i=1}^{m}f(x_{i})\ln{}f(x_{i})+\lambda_{0}\sum_{i=1}^{m}f+\lambda_{1}\sum_{i=1}^{m}r(x_{1})f(x_{i})
\end{equation}
and differentiate it with respect to $x_{i}$
\begin{equation}
  \label{eq:funder}
\frac{\partial{}J}{\partial{}f(x_{i})}=-\ln(f(x_{i}))-1+\lambda_{0}+\lambda_{1}r(x_{i})
\end{equation}

Set ({\ref{eq:funder}}) to $0$ to find the necessary condition to
maximize ({\ref{eq:entropy}})
\begin{equation}
  \label{eq:coverthomas}
g(x_{i})=e^{\lambda_{0}-1+\lambda_{1}r(x_{i})}
\end{equation}

This is the Gibbs distribution. We still need to do two things: (a)
show that the entropy of $g$ is maximal, and (b) show how to find
$\lambda_{0}$ and $\lambda_{1}$. (a) is shown in Theorem 12.1.1 in
Cover and Thomas \scite{1}{coverthomas06}{} and there is no reason to
copy it here. I was not able to find (b) in the literature and will
show it here. A faint suggestion on how to do it is in Jaynes'
treatment of the Brandeis Dice Problem (see \scite{8}{jaynes89}{243}). 

Let
\begin{equation}
  \label{eq:l1}
\lambda_{1}=-\beta
\end{equation}
\begin{equation}
  \label{eq:zet}
Z(\beta)=\sum_{i=1}^{m}e^{-\beta{}r(x_{i})}
\end{equation}
\begin{equation}
  \label{eq:l0}
\lambda_{0}=1-\ln(Z(\beta))
\end{equation}

To find $\lambda_{0}$ and $\lambda_{1}$ we introduce the constraint
\begin{equation}
  \label{eq:logcon}
-\frac{\partial}{\partial{}\beta}\ln(Z(\beta))=\alpha
\end{equation}

To see how this constraint gives us $\lambda_{0}$ and $\lambda_{1}$
Jaynes' solution of the Brandeis Dice Problem is a helpful example. We
are, however, interested in a general proof that this choice of
$\lambda_{0}$ and $\lambda_{1}$ gives us the probability distribution
maximizing the entropy. That $g$ so defined maximizes the entropy is
shown in (a). We need to make sure, however, that with this choice of
$\lambda_{0}$ and $\lambda_{1}$ the constraints
({\ref{eq:constraint}}) and ({\ref{eq:unity}}) are also fulfilled.

First, we show
% \begin{align}
% &\mathbb{R}^{3}\mbox{ can be endowed with a metric }l_{2}\notag
% \\
% &\mbox{with constant positive curvature }K=k\label{carmor2}\tag{C2}
% \end{align}
\begin{align}
&\sum_{i=1}^{m}g(x_{i})=\sum_{i=1}^{m}e^{\lambda_{0}-1+\lambda_{1}r(x_{i})}=e^{\lambda_{0}}\sum_{i=1}^{m}e^{\lambda_{1}r(x_{i})}=\notag\\
&e^{-\ln(Z(\beta))}Z(\beta)=1\label{eq:unishow}
\end{align}

Then, we show, by differentiating $\ln(Z(\beta))$ using the
substitution $x=e^{-\beta}$
\begin{align}
&\alpha=-\frac{\partial}{\partial{}\beta}\ln(Z(\beta))=-\frac{1}{\sum_{i=1}^{m}x^{r(x_{i})}}\left(\sum_{i=1}^{m}r(x_{i})x^{r(x_{i})-1}\right)(-x)=\notag\\
&\frac{\sum_{i=1}^{m}r(x_{i})x^{r(x_{i})}}{\sum_{i=1}^{m}x^{r(x_{i})}}\label{eq:alphashow}
\end{align}

And, finally,
\begin{align}
&\sum_{i=1}^{m}r(x_{i})g(x_{i})=\sum_{i=1}^{m}r(x_{i})e^{\lambda_{0}-1+\lambda_{1}r(x_{1})}=e^{\lambda_{0}-1}\sum_{i=1}^{m}r(x_{i})e^{\lambda_{1}r(x_{1})}=\notag\\
&e^{\lambda_{0}-1}\sum_{i=1}^{m}r(x_{i})x^{r(x_{i})}=\alpha{}e^{\lambda_{0}-1}\sum_{i=1}^{m}x^{r(x_{i})}=\alpha{}e^{\lambda_{0}-1}\sum_{i=1}^{m}e^{-\beta{}r(x_{i})}=\notag\\
&\alpha{}Z(\beta)e^{\lambda_{0}-1}=\alpha{}Z(\beta))e^{-\ln(Z(\beta))}=\alpha\label{eq:finalshow}
\end{align}

\kapt{The Full Employment Theorem}

The Kullback-Leibler Divergence is not a metric: it is not symmetric
and does not obey the triangle inequality. (Point out Uffink's average
mistake.) \textsc{minxent} requires that $Q$ be the probability
distribution fulfilling the constraint ({\ref{eq:constraint}}) and
minimizing the Kullback-Leibler Divergence $D(Q,P)$. 
\begin{equation}
  \label{eq:kl}
  D(Q,P)=\sum_{i=1}^{m}q_{i}\log\frac{q_{i}}{p_{i}}
\end{equation}

We can either use the constraint rule to calculate $v_{1}$ (see Green
Book 141 and 143) or differentiate $D(Q,P)$ directly for its critical
points and make $G$ explicit (see Green Book 148).
\begin{equation}
  \label{eq:fromq1toq}
  G(q)=q_{1}=\frac{C}{1+Ct+C}
\end{equation}
where
\begin{equation*}
  t=\frac{q}{1-q}
\end{equation*}
and
\begin{equation*}
  C=2^{-\frac{t\log{}t+t+1}{1+t}}
\end{equation*}

In his (somewhat) recent book, \emph{Reasoning About Uncertainty},
Joseph Halpern writes:
\begin{quotex}
  It is perhaps not surprising that there are proponents of maximum
  entropy and relative entropy who recommend that if an agent's
  information can be characterized by a set $C$ of constraints, then
  the agent should act \qnull{as if} the probability is determined by
  the measure that maximizes entropy relative to $C$ (i.e., the
  measure that has the highest entropy of all the measures in $C$).
  Similarly, if the agent starts with a particular measure $\mu$ and
  gets new information characterized by $C$, he should update to the
  measure $\mu'$ that satisfies $C$ such that the relative entropy
  between $\mu'$ and $\mu$ is a minimum. Maximum entropy and relative
  entropy have proved quite successful in a number of applications,
  from physics to natural-language modeling. Unfortunately, they also
  exhibit some counterintuitive behavior on certain applications.
  Although they are valuable tools, they should be used with care.
  \scite{3}{halpern03}{110}
\end{quotex}

The principle example for this counterintuitive behaviour in Halpern
(for another one treated in \scite{7}{uffink96}{}, section 5, see
\scite{7}{diasshimony81}{}) is the Judy Benjamin case in van Fraassen
\scite{1}{fraassen81}{}. Halpern has some personal investment in this
case, as he wrote a paper defending a more classical Bayesian approach
to the case (see \scite{7}{grovehalpern97}{}). In this paper, Grove
and Halpern want to save intuition (I1) with assumptions that
guarantee the independence of $P_{\mbox{\tiny{HQ}}}(A_{3})$ and
$P_{\mbox{\tiny{HQ}}}(A_{2}|A_{1}\cup{}A_{2})$ (the superscript HQ
stands for headquarters and emphasizes that Judy Benjamin conditions
on the information received by headquarters without necessarily making
headquarters' beliefs her own). Indeed, as they set it up, $q_{3}$
remains at $1/2$, contrary to (I2), the normalized odds vector $v_{1}$
and the function $G$ we just provided.

Both in the paper and in the book, the Judy Benjamin case is a special
case in the more general argument that \qeins{the only right way to
  update, especially in an incompletely specified situation, is to
  think very carefully about the real nature and origin of the
  information we receive} \scite{2}{grovehalpern97}{3}. Although they
do not use the terminology, Grove and Halpern advocate a type of Full
Employment Theorem (used in computer science, where due to Turing's
halting problem the computer scientist cannot be dispensed with in the
writing of computer programs). They end their paper with the call that
\qeins{one must always think carefully about precisely what the
  information means} \scite{2}{grovehalpern97}{6}.

Halpern's book is a more large-scale presentation of pluralism in
reasoning about uncertainty. Possible worlds, probability measures,
lower and upper probabilities, Dempster-Shafer belief functions,
possibility measures, ranking functions, relative likelihoods,
and plausibility measures are introduced, concluding that it is in the
end up to the inquirer to choose a representation of uncertainty that
fits the bill: \qeins{there is no escaping the need to understand the
  details of the application} \scite{2}{halpern03}{423}.

This contrasts with physical statistics where \textsc{maxent} is often
considered to be a uniquely consistent method of integrating ignorance
and information. The uniqueness claim, for example advocated in
\scite{4}{shorejohnson80}{}, is contested (see, for example, Uffink
\scite{7}{uffink95}{}, but also van Fraassen in
\scite{7}{fraassenetal86}{}), where the authors suggest
\textsc{maxent} to be a special instance of a family of principles
which are consistent relative to specified assumptions). It is widely
used in physics, where much effort has gone into showing the
consistency of \textsc{maxent} and Bayesian updating (see
\scite{7}{catichagiffin06}{}, where the family of Renyi functions
proposed by Uffink are ruled out, and \scite{7}{giffin08}{}, where the
author seeks to \qeins{show that \textsc{maxent} is capable of
  producing every aspect of orthodox Bayesian inference and prove the
  complete compatibility of Bayesian and entropy methods}). (For
\textsc{maxent} in peaceful coexistence with, conflict with, or as a
generalization of Bayesian conditionalization see
\scite{7}{uffink96}{}, for Bayesian conditionalization as a
generalization of \textsc{maxent} see \scite{7}{skyrms85}{}.)

We return to Judy Benjamin. Applying \textsc{minxent}, we found
intuition (I2) confirmed (van Fraassen). As we pointed out at the
beginning of this section, however, the Kullback-Leibler Divergence is
not symmetric. We are better off calling it the divergence of $Q$ from
$P$, rather than the distance of $Q$ and $P$ in terms of information.
Symmetric expressions of relative entropy exist, but they do not have
some of the attractive properties of the Kullback-Leibler Divergence
(in fact, Kullback and Leibler themselves suggested the symmetric
relative entropy $D(P,Q)+D(Q,P)$). In statistics, this asymmetry is
often less problematic because the divergence is understood to be
\emph{from} a model \emph{to} reality. The asymmetry is justified by
the different nature of $Q$ (the model) and $P$ (reality).

Is the asymmetry justified in Judy Benjamin's case? Judy's prior
probability distribution is expressed in the normalized odds vector
$v_{0}=(.25,.25,.5)$. Then she receives the information from
headquarters that $Q(A_{1}|A_{1}\cup{}A_{2})=.75$. But perhaps she
proceeds in reverse order: first she learns that the chance of being
in $A_{2}$ is three times the chance of being in $A_{1}$ from
headquarters, without a prior conception of how large
$A_{1},A_{2},A_{3}$ are. She then uses \textsc{maxent} rather than
\textsc{minxent} to establish her first probability distribution
conditioned on the information she received from her headquarters:
\begin{equation}
  \label{eq:maxentjudy}
  v_{2}=(.16,.48,.36)
\end{equation}

To achieve this result we use the constraint rule with the constraint
$r_{1}=0,r_{2}=1,r_{3}=q$ (this is equivalent to $q_{1}=3q_{2}$ in
Judy Benjamin's case where $q=3/4$). (Reader beware, whenever we use
the constraint rule there are some hefty calculations.)
\begin{equation}
  \label{eq:revconlambda0}
  \lambda_{0}=e^{-\frac{q}{1-q}}+e^{-\frac{q^{2}}{1-q}}\mbox{ and }\lambda_{1}=-\frac{q}{1-q}
\end{equation}

Notice that the third element of $v_{2}$ is already $>1/3$, in keeping
with intuition (I2). Only now Judy Benjamin considers the proportion
of the probabilities corresponding to the size of the $A_{i}$ ($A_{3}$
is twice the size of $A_{1}$ and $A_{2}$). To use the constraint rule,
we like to express our constraints in terms of the expectation
$\sum_{i=1}^{m}r(x_{i})q(x_{i})$. In this case, we find that
$r_{1}=2,r_{2}=2,r_{3}=0$ is equivalent to the constraint just
formulated. Again, using the constraint rule
\begin{equation}
  \label{eq:revconlambda1}
  \lambda_{0}=1-\ln{}2\mbox{ and }\lambda_{1}=-\frac{1}{2}\ln{}2
\end{equation}

This provides us with the normalized odds vector
$v_{3}=(.125,.375,.5)$, which is just what (I1) requires! It is a case
of which of the two pieces of information we put first whether (I1) or
(I2) prevails. If we change $q=3/4$ to a variable we have
$v_{3}=(.5(1-q),.5q,.5)$. The continuity required by (I2) is also
fully restored. The results for the Judy Benjamin case are not at all
counterintuitive as Halpern suggests, but as in the case of Bayesian
conditionalization the sequence matters by which we use the
information we have.

\kapt{Some Open Questions}

Why does Dempster's Rule of Combination give us \textsc{minxent} with
respect to $D(Q,P)$, not $D(P,Q)$? Dempster's Rule of Combination is
symmetrical and presumes that the two pieces of information are
independent. Next: give an example of why sequence matters in Bayesian
updating, and show that it is not counterintuitive. Address Halpern
and Grove's assumptions which favour (I1). Address Friedman and
Shimony's counterexample to \textsc{maxent}. What is the matter with
the graph for $G$? Label the two pieces of information \emph{map} and
\emph{hq}. 

% \noindent\textbf{Endnotes}

% \theendnotes

% \medskip

% \noindent\textbf{References}

% \nocite{*} 
\bibliographystyle{stefan-2010-08-28}
\bibliography{bib-3306}

\end{document}

% The Kullback-Leibler divergence,
% however, is not a metric, and is in particular not symmetric.
% \begin{displaymath}
%   D(Q,P)\neq{}D(P,Q)
% \end{displaymath}
% Thus, we call it the divergence \emph{of} $Q$ \emph{from} $P$ rather
% than the distance between $Q$ and $P$. It is often understood to be
% the divergence of a model from reality. In our case, however,
% ({\ref{eq:map}}) and ({\ref{eq:hdq}}) have no such connotations and do
% not need to be applied in this sequence. True, it seems more natural
% that Judy considers ({\ref{eq:map}}) first and then calculates her
% posteriors upon hearing ({\ref{eq:hdq}}). As we have seen, this
% accords with intuition (S2), which may be the better intuition.

% If she first processes the information received from her headquarters,
% however, and then applies the knowledge of ({\ref{eq:map}}) to yield
% her posteriors, the Kullback-Leibler divergence to minimize would be
% $D(P,Q)$.