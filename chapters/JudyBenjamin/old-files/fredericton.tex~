\documentclass[xcolor=dvipsnames]{beamer}

\usepackage{graphicx}
\usepackage{wrapfig}

\mode<presentation>
{
  \usetheme{Warsaw}
  \setbeamercovered{transparent}
}
% \usecolortheme[named=OliveGreen]{structure}
\setbeamertemplate{navigation symbols}{} 
\setbeamertemplate{blocks}[rounded][shadow=true] 

\title{Judy Benjamin and Halpern's \linebreak Full Employment Theory}
\subtitle{Annual Meeting of the Canadian Society \linebreak
  for the History and Philosophy of Science}

\author{Stefan Lukits}

\date{May 29, 2011}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
  \frametitle{Halpern's Full Employment Theorem}
  \begin{columns}
    \column{.50\textwidth}{
Choosing a representation:
    \begin{itemize}
    \item possible worlds
    \item probability measures
    \item lower and upper probabilities
    \item Dempster-Shafer belief functions
    \item possibility measures
    \item ranking functions
    \item relative likelihoods
    \item plausibility measures
    \end{itemize}}
\column{.50\textwidth}{
  \begin{block}{Grove and Halpern}
    ``one must always think carefully about precisely what the
    information means'' \linebreak (``Probability Update \ldots,'' p6)
  \end{block}
  \begin{block}{Joseph Halpern}
    ``there is no escaping the need to understand the details of the
    application'' \linebreak (\emph{Reasoning \ldots}, p423)
  \end{block}
}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Pluralism Versus Statistical Physics I}
  \begin{block}{Statistical Physics}
    ``Jaynes's principle of maximum entropy and Kullbacks
    principle of minimum cross-entropy (minimum directed
    divergence) are shown to be uniquely correct methods for
    inductive inference when new information is given in the form
    of expected values.'' (Shore and Johnson)
  \end{block}
  \begin{block}{Pluralism}
    ``The uniqueness proofs are flawed, or rest on unreasonably
    strong assumptions. A more general class of inference rules,
    maximizing the so-called R{\'e}nyi entropies, is exhibited
    which also fulfill the reasonable part of the consistency
    assumptions.'' (Jos Uffink)
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Pluralism Versus Statistical Physics II}
  \begin{block}{Statistical Physics}
    ``We show that Skilling's method of induction leads us to a
    unique general theory of inductive inference, the maximum
    entropy method, and precisely how it is that other entropies
    such as those of R{\'e}nyi or Tsallis are ruled out for
    problems of inference. We then explore the compatibility of
    Bayes and maximum entropy updating. We show that maximum
    entropy is capable of producing every aspect of orthodox
    Bayesian inference and prove the complete compatibility of
    Bayesian and entropy methods.'' (Adom Giffin)
 \end{block}
\end{frame}

\begin{frame}
  \frametitle{Pluralism Versus Statistical Physics III}
\begin{block}{Pluralism}
  ``It is perhaps not surprising that there are proponents of maximum
  entropy and relative entropy who recommend that if an agent's
  information can be characterized by a set $C$ of constraints, then
  the agent should act `as if' the probability is determined by the
  measure that maximizes entropy relative to $C$ (i.e., the measure
  that has the highest entropy of all the measures in $C$). Similarly,
  if the agent starts with a particular measure $\mu$ and gets new
  information characterized by $C$, he should update to the measure
  $\mu'$ that satisfies $C$ such that the relative entropy between
  $\mu'$ and $\mu$ is a minimum. Maximum entropy and relative entropy
  have proved quite successful in a number of applications, from
  physics to natural-language modeling. Unfortunately, they also
  exhibit some counterintuitive behavior on certain applications.
  Although they are valuable tools, they should be used with care.''
  \linebreak (Joseph Halpern)
\end{block}
\end{frame}

\begin{frame}
  \frametitle{Objective}
\begin{itemize}
\item[$\rightarrow$] Halpern claims that the principle of maximum entropy
  sometimes exhibits counterintuitive behaviour on certain
  applications. Therefore, the principle of full employment
  applies.
\item[$\rightarrow$] Halpern singles out the Judy Benjamin case to demonstrate
  the counterintuitive behaviour of maximum entropy.
\item[$\rightarrow$] We will show that the Judy Benjamin case does not behave
  counterintuitively when approached via maximum entropy. On the
  contrary, maximum entropy reveals where the information given by
  the case is incompletely specified and provides precisely the
  results that correspond to our intuitions.
\item[$\rightarrow$] Consequently, Halpern does not have a strong case for the
  full employment theory with respect to maximum entropy. This
  supports our larger project of a carefully specified
  epistemological primacy of information theory in matters of
  uncertainty.
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The Judy Benjamin Case I}
\begin{figure}[h]
\includegraphics[scale=.5]{judy.pdf}
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{The Judy Benjamin Case II}
\begin{itemize}
\item<1-> (MAP) Judy has no idea where she is. She is on team Blue.
  Because of the map, her probability of being in Blue territory
  equals the probability of being in Red territory, and being on Red
  Second Company ground equals the probability of being on Red
  Headquarters ground.
\item<2-> (HDQ) Headquarters inform Judy that in case she is in Red
  territory, her chance of being on their Headquarters ground is three
  times the chance of being on their Second Company ground.
\end{itemize}
\pause
\pause
\begin{equation}
  \label{eq:map}
  2\cdot{}P(A_{1})=2\cdot{}P(A_{2})=P(A_{3})\tag{\mbox{MAP}}
\end{equation}
\pause
\begin{equation}
  \label{eq:hdq}
  q=P(A_{2}|A_{1}\cup{}A_{2})=\frac{3}{4}\tag{\mbox{HDQ}}
\end{equation}
\end{frame}

\begin{frame}
  \frametitle{Bayesian Updating}
  \color{blue}{Why Bayesian Updating might not work:}\pause
\begin{itemize}
  \item Bayesian updating requires that an event
    $T\subseteq{}\Omega$ is true.
  \item Sometimes, however, the information received assigns a
    probability to $T$ or a value to the expectation of a random
    variable. We will see that the former case can always be reduced
    to the latter.
  \item Grove and Halpern try to save Bayesian Updating for the
    Judy Benjamin case by setting $T=$``receiving information HDQ.''
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Two Intuitions}
  \begin{itemize}
  \item<1->[\color{blue}{(S1)}] HDQ should not affect $P(A_{3})$. Let's
    call $P$ the prior probability distribution, before receiving HDQ,
    and $Q$ the posterior probability distribution, after receiving
    HDQ. Then $Q(A_{3})=P(A_{3})$.
  \item<2->[\color{blue}{(S2)}] If the value of $q$ approaches $1$
  then $Q(A_{3})$ should approach $2/3$. HDQ would then be ``if you
  are in Red territory you are almost certainly on Red Headquarters
  ground.'' Considering MAP, $Q(A_{3})$ should approach $2/3$.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{MAXENT and MINXENT I}
Depending on whether we have a prior probability distribution we can
use \textsc{maxent} or \textsc{minxent} to calculate the posterior
probability distribution using maximum entropy.
\begin{description}
\item[\textsc{maxent}] Choose the probability distribution that
  fulfills the constraint provided by the information you have and is
  otherwise maximally uncertain (maximum entropy).
\item[\textsc{minxent}] Choose the probability distribution that
  fulfills the constraint provided by the information you have and is
  otherwise minimally discriminate from the given prior probability
  distribution (minimum cross-entropy).
\end{description}
\end{frame}

\begin{frame}
  \frametitle{MAXENT and MINXENT II}
  It is most expedient to provide constraints in the form of
  expectations. We define a payoff function $r$ and then set the
  expectation to a value $\alpha$. To be slightly more general, we
  shall call our events $x_{i}$ instead of $A_{i}$, and $m=3$. Let
  $p_{i}=P(A_{i})$ and $q_{i}=Q(A_{i})$.
  \begin{equation}
    \label{eq:map1}
    r(x_{1})=2,r(x_{2})=2,r(x_{3})=0,\alpha=\sum_{i=1}^{m}r(x_{i})q_{i}=1\tag{MAP}
  \end{equation}
  \begin{equation}
    \label{eq:hdq1}
    r(x_{1})=0,r(x_{2})=1,r(x_{3})=q,\alpha=\sum_{i=1}^{m}r(x_{i})q_{i}=q\tag{HDQ}
  \end{equation}
\end{frame}

\begin{frame}
  \frametitle{MAXENT and MINXENT III}
  For \textsc{maxent}, we want to maximize the Shannon entropy:
  \begin{equation}
    \label{eq:maxentfin}
    H(Q)=-\sum_{i=1}^{m}q_{i}\ln{}q_{i}
  \end{equation}
  For \textsc{minxent}, we want to minimize the Kullback-Leibler
  divergence, given first in its general form as a Lebesgue integral
  ($\mu$ is any background measure for which the Radon-Nikodym
  derivatives of $Q$ and $P$ exist), then more applicably to the Judy
  Benjamin case as a finite sum:
  \begin{equation*}
    \label{eq:minxentleb}
    D(Q||P)=\int_{\Omega}q\ln\frac{q}{p}d\mu
  \end{equation*}
  \begin{equation}
    \label{eq:minxentfin}
    D(Q||P)=\sum_{i=1}^{m}q_{i}\ln\frac{q_{i}}{p_{i}}d\mu
  \end{equation}
\end{frame}

\begin{frame}
  \frametitle{The Constraint Rule for \textsc{maxent} I}
Let $f$ be a probability distribution on a finite space
$x_{1},\ldots,x_{m}$ that fulfills the constraint
\begin{equation}
  \label{eq:constraint}
\sum_{i=1}^{m}r(x_{i})f(x_{i})=\alpha
\end{equation}

There are two constraints for $f$:
\begin{equation}
  \label{eq:unity}
\sum_{i=1}^{m}f(x_{i})=1
\end{equation}
\begin{equation}
  \label{eq:entropy}
\mbox{maximize }-\sum_{i=1}^{m}f(x_{i})\ln(x_{i})
\end{equation}
\end{frame}

\begin{frame}
  \frametitle{The Constraint Rule for \textsc{maxent} II}
We use Lagrange multipliers to define the functional
\begin{equation}
  \label{eq:functional}
J(f)=-\sum_{i=1}^{m}f(x_{i})\ln{}f(x_{i})+\lambda_{0}\sum_{i=1}^{m}f+\lambda_{1}\sum_{i=1}^{m}r(x_{1})f(x_{i})
\end{equation}
and differentiate it with respect to $x_{i}$
\begin{equation}
  \label{eq:funder}
\frac{\partial{}J}{\partial{}f(x_{i})}=-\ln(f(x_{i}))-1+\lambda_{0}+\lambda_{1}r(x_{i})
\end{equation}
\end{frame}


\begin{frame}
  \frametitle{The Constraint Rule for \textsc{maxent} III}
Set ({\ref{eq:funder}}) to $0$ to find the necessary condition to
maximize ({\ref{eq:entropy}})
\begin{equation}
  \label{eq:coverthomas}
g(x_{i})=e^{\lambda_{0}-1+\lambda_{1}r(x_{1})}
\end{equation}

This is the Gibbs distribution. We need to (a) show that the entropy
of $g$ is maximal, and (b) show how to find $\lambda_{0}$ and
$\lambda_{1}$. (a) is part of the proof, but we won't need it later
on. We will only show (b). 
\end{frame}

\begin{frame}
  \frametitle{The Constraint Rule for \textsc{maxent} IV}
  Let
  \begin{equation}
  \label{eq:l1}
\lambda_{1}=-\beta
\end{equation}
\begin{equation}
  \label{eq:zet}
Z(\beta)=\sum_{i=1}^{m}e^{-\beta{}r(x_{i})}
\end{equation}
\begin{equation}
  \label{eq:l0}
\lambda_{0}=1-Z(\beta)
\end{equation}

To find $\lambda_{0}$ and $\lambda_{1}$ we set
\begin{equation}
  \label{eq:logcon}
-\frac{\partial}{\partial{}\beta}\ln(Z(\beta))=\alpha
\end{equation}
\end{frame}

\begin{frame}
  \frametitle{The Constraint Rule for \textsc{maxent} V}
  That $g$ so defined maximizes the entropy is shown in (a). We need
  to make sure, however, that with this choice of $\lambda_{0}$ and
  $\lambda_{1}$ the constraints ({\ref{eq:constraint}}) and
  ({\ref{eq:unity}}) are also fulfilled.

  First, we show
  \begin{align}
    &\sum_{i=1}^{m}g(x_{i})=\sum_{i=1}^{m}e^{\lambda_{0}-1+\lambda_{1}r(x_{i})}=e^{\lambda_{0}}\sum_{i=1}^{m}e^{\lambda_{1}r(x_{i})}=\notag\\
    &e^{-\ln(Z(\beta))}Z(\beta)=1\label{eq:unishow}
  \end{align}
\end{frame}

\begin{frame}
  \frametitle{The Constraint Rule for \textsc{maxent} VI}
  Then, we show, by differentiating $\ln(Z(\beta))$ using the
  substitution $x=e^{-\beta}$
  \begin{align}
    &\alpha=-\frac{\partial}{\partial{}\beta}\ln(Z(\beta))=-\frac{1}{\sum_{i=1}^{m}x^{r(x_{i})}}\left(\sum_{i=1}^{m}r(x_{i})x^{r(x_{i})-1}\right)(-x)=\notag\\
    &\frac{\sum_{i=1}^{m}r(x_{i})x^{r(x_{i})}}{\sum_{i=1}^{m}x^{r(x_{i})}}\label{eq:alphashow}
  \end{align}

  And, finally,
  \begin{align}
    &\sum_{i=1}^{m}r(x_{i})g(x_{i})=\sum_{i=1}^{m}r(x_{i})e^{\lambda_{0}-1+\lambda_{1}r(x_{1})}=e^{\lambda_{0}-1}\sum_{i=1}^{m}r(x_{i})e^{\lambda_{1}r(x_{1})}=\notag\\
    &e^{\lambda_{0}-1}\sum_{i=1}^{m}r(x_{i})x^{r(x_{i})}=\alpha{}e^{\lambda_{0}-1}\sum_{i=1}^{m}x^{r(x_{i})}=\alpha{}e^{\lambda_{0}-1}\sum_{i=1}^{m}e^{-\beta{}r(x_{i})}=\notag\\
    &\alpha{}Z(\beta)e^{\lambda_{0}-1}=\alpha{}Z(\beta))e^{-\ln(Z(\beta))}=\alpha\label{eq:finalshow}
  \end{align}
\end{frame}

\begin{frame}
  \frametitle{Intuitions and Judy Benjamin I}
  The normalized odds vector for the Judy Benjamin case without using
  information ({\ref{eq:hdq}}) is:
  \begin{displaymath}
    v_{0}=(.25,.25,.5)
  \end{displaymath}

  Let's apply the constraint rule to the Judy Benjamin case. Instead
  of proving an analogous procedure for \textsc{minxent}, we first
  calculate the posterior using the information for ({\ref{eq:hdq}}),
  but not yet for ({\ref{eq:map}}), so that we can use the procedure
  for \textsc{maxent} that we just spelled out.
  ({\ref{eq:constraint}}) is defined by ({\ref{eq:hdq1}}). The lambdas
  are:
  \begin{align}
    &\lambda_{0}=1-\ln(\sum_{i=1}^{m}e^{\lambda_{1}r(x_{i})})\notag
    &\lambda_{1}=\ln{}q-\ln(1-q)\notag
  \end{align}
\end{frame}

\begin{frame}
  \frametitle{Intuitions and Judy Benjamin II}
  We combine the normalized odds vector following from these lambdas
  ($v_{2}=(.16,.48,.36)$) using Dempster's Rule of Combination with
  ({\ref{eq:map}}) and get
  \begin{displaymath}
    v_{1}=(.12,.35,.53)
  \end{displaymath}

  You can check this result by differentiating and examining the
  critical points of the Kullback-Leibler divergence directly. It
  contradicts intuition (S1)
\end{frame}

\begin{frame}
  \frametitle{Plotting $q_{1}$ as a Function of $q$}
  \begin{figure}[h]
    \includegraphics[scale=.5]{plotq1.pdf}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{End of Presentation}
\end{frame}

\end{document}