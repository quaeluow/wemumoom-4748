* ideas
** email to Jan-Willem Romeijn
Ironically, I don't even think I am done responding to your paper.
What you say about epistemic entrenchment, Hellinger's Distance etc.
sounds super-plausible, but it's really incompatible with the
Principle of Maximum Entropy (not that you would mind, as you don't
subscribe to it). I'd say that philosophers who know something about
this are currently 99:1 in your favour that besides a probability
distribution one also needs to have some kind of epistemic
entrenchment in order to update probabilities uniquely following an
observation. This is especially well-articulated in Spohn's book on
ranking functions. It's almost like there is a first layer reflecting
my probability distribution and then a second layer reflecting to what
degree I am entrenched about these probabilities. If I want to defend
the PME, I must show that this second layer is really either part of
the background information or part of the new evidence. It is
evidential, not epistemological. When I say 99:1 I mean that I haven't
found a single philosopher yet who shares my intuition that the PME
does not need to be guarded (and constrained) by the labour of
epistemologists, epistemologists just need to understand it better.
But there are quite a few physicists ...
** see extensive notes in Schmierbuch, beginning p767
Solution for the Linguist using Bertsekas on page 783. Pair it down to
three element partitions p795. Pair it down to two p801. 
** use Sundowners at the Westcliff
see page 804 in Schmierbuch.

Sarah and Marian have arranged to go for sundowners at the Westcliff
hotel tomorrow. Sarah feels there is some chance that it will rain,
but thinks they can always enjoy the view from inside. To make sure,
Marian consults the staff at the Westcliff hotel and finds out that in
the event of rain, the inside area will be occupied by a wedding
party. So she tells Sarah: ``If it rains tomorrow, we cannot have
sundowners at the Westcliff.'' Upon learning this conditional, Sarah
sets her probability for sundowners and rain to zero, but she does not
adapt her probability for rain.

A jeweller has been shot in his store and robbed of a golden watch.
How- ever, it is not clear at this point what the relation between
these two events is; perhaps someone shot the jeweller and then
someone else saw an opportunity to steal the watch. Kate thinks there
is some chance that Henry is the robber (R). On the other hand, she
strongly doubts that he is capable of shooting someone, and thus, that
he is the shooter (S). Now the inspector, after hearing the
testimonies of several witnesses, tells Kate: ``If Henry robbed the
jeweller, then he also shot him.'' As a result, Kate becomes more
confident that Henry is not the robber, while her probability for
Henry having shot the jeweller does not change.
* physics forum questions
** 2014-02-18
It is well-known that with known marginal probabilities a_{i} and
b_{j} the joint probability distribution maximizing the entropy

[tex]H(P)=-\sum_{i=1}^{m}\sum_{j=1}^{n}p_{ij}\log{}p_{ij}[/tex]

is [tex]p_{ij}=a_{i}b_{j}[/tex] 

For m=3 and n=3, a=(0.2,0.3,0.5), b=(0.1,0.6,0.3), for example,

[tex]\begin{equation}
  \label{eq:r1}
P=\left(
  \begin{array}{rrr}
    0.02 & 0.12 & 0.06 \\
    0.03 & 0.18 & 0.09 \\
    0.05 & 0.30 & 0.15
  \end{array}
\right)
\end{equation}[/tex]

jNow, I have a problem where the joint probability distribution is
constrained (much like a random walk where the path from one node to
another is blocked). For example, let m, n, a, and b be as above with
the constraint that

[tex]\begin{equation}
  \label{eq:r2}
P=\left(
  \begin{array}{rrr}
    p_{11} & 0 & p_{13} \\
    p_{21} & p_{22} & p_{23} \\
    p_{31} & p_{32} & p_{33} \\
  \end{array}
\right)
\end{equation}[/tex]

Because a and b are known, it suffices to find out any 2x2 matrix
contained in the 3x3 matrix, for example (x,y,z the variables by which
w_{1}, w_{2}, v_{1}, v_{2}, and sigma are expressible, given a and b)

[tex]\begin{equation}
  \label{eq:r3}
P=\left(
  \begin{array}{rrr}
    x & 0 & w_{1} \\
    y & z & w_{2} \\
    v_{1} & v_{2} & \sigma \\
  \end{array}
\right)
\end{equation}[/tex]

I use this to write out the entropy and differentiate with respect to
x, y, and z to find out that the maximum will be where

[tex]\begin{equation}
  \label{eq:r4}
  \det\left(
    \begin{array}{ll}
      x & w_{1} \\
      v_{1} & \sigma
    \end{array}
\right)=0
\end{equation}[/tex]

[tex]\begin{equation}
  \label{eq:r5}
  \det\left(
    \begin{array}{ll}
      y & w_{2} \\
      v_{1} & \sigma
    \end{array}
\right)=0
\end{equation}[/tex]

[tex]\begin{equation}
  \label{eq:r6}
  \det\left(
    \begin{array}{ll}
      z & w_{2} \\
      v_{2} & \sigma
    \end{array}
\right)=0
\end{equation}[/tex]

This is a system of 3 non-linear equations which are awkward to solve
algebraically. In the end, I am interested to know which property a
and b need to have to ensure that P is a proper probability
distribution (i.e. no negative elements). For now, however, I'd be
thrilled if anybody could give me a hint how I could find the
solutions for x, y, and z algebraically without these non-linear
equations.
* cut sections
** Old Intro
There are problems of probability update which cannot be addressed
effectively by means of standard conditioning or Jeffrey conditioning.
Usually, the evidence (or observation, or new information) arises in
the form of an event (standard conditioning) or the redistribution of
probabilities over a complete partition of events (Jeffrey
conditioning). Sometimes the evidence arises in the form of a
constraint which falls into neither of the above categories. Bas van
Fraassen's \emph{Judy Benjamin} problem and E.T. Jaynes'
\emph{Brandeis Dice} problem are two examples. 

To solve these cases, Jaynes suggests the principle of maximum
entropy, which extends the idea of optimal information processing
(which standard conditioning and Jeffrey conditioning obey) to a
larger class of constraints. He originally developed the principle of
maximum entropy as a synchronic norm (call this synchronic norm
\textsc{maxent}), where constraints are coordinated with
non-informative prior probabilities to result in a probability
distribution or density whose entropy is maximal (using Shannon's
information entropy) but which at the same time obeys all the
constraints.

Soon after Jaynes, P.M. Williams and Bas van Fraassen (see
\scite{7}{williams80}{} and \scite{7}{fraassen81}{}) applied the
principle of maximum entropy to problems of probability update where
the word \qnull{prior} is used comparatively and refers to a
probability distribution which precedes new information and therefore
the posterior probability distribution; it is not used superlatively
and does not refer to a probability distribution which precedes any
information at all or has ambitions to be non-informative. Here we are
concerned with what Richard Jeffrey terms \qnull{probability
  kinematics,} the rules or guidelines when moving from a given prior
probability distribution to a posterior probability distribution in
the wake of new information (call this diachronic norm
\emph{Infomin}). The Kullback-Leibler divergence is used to extend the
Shannon entropy from \textsc{maxent} to \emph{Infomin}.

Sometimes the case is made that \textsc{maxent} and \emph{Infomin} are
two different norms prescribing different probability distributions in
certain cases. Consider a bag with blue, green, and red tokens. You
know that (C1) at least 50\% of the tokens are blue. Then you learn
that (C2) at most 20\% of the tokens are red. The synchronic norm
\textsc{maxent}, on the one hand, ignores the diachronic
dimension and prescribes the probability distribution which has the
maximum entropy and obeys both (C1) and (C2). The three-dimensional
vector containing the probabilities for blue, green, and red is
$(\frac{1}{2},\frac{1}{5},\frac{3}{10})$. \emph{Infomin}, on the other
hand, takes as its prior probability distribution
$(\frac{1}{2},\frac{1}{4},\frac{1}{4})$ and then diachronically updates
to $(\frac{8}{15},\frac{1}{5},\frac{4}{15})$. 

While it is useful to distinguish between synchronic and diachronic
norms, one cannot drive a wedge between \textsc{maxent} and
\emph{Infomin}. The information provided in a problem calling for
\textsc{maxent} and the information provided in a problem calling for
\emph{Infomin} is different, as temporal relations and their
implications for dependence between variables clearly matter. In the
above case, we might have relevantly received information (C2) before
(C1) (and \qnull{before} may be understood logically rather than
temporally) so that \emph{Infomin} updates
$(\frac{1}{2},\frac{1}{4},\frac{1}{4})$ to
$(\frac{1}{2},\frac{1}{6},\frac{1}{3})$. Even if (C1) and (C2) are
received in a definite order, the problem may be phrased in a way that
indicates independence between the two constraints. In this case,
\textsc{maxent} is the appropriate norm to use. \emph{Infomin}
correctly does not assume such independence and therefore processes
the two pieces of information separately. For the rest of the article,
we will therefore assume that \emph{Infomin} is merely a proper
extension of \textsc{maxent} to probability kinematics and refer to
the principle under debate (which we will define in more detail
further below) as the \textsc{pme} (the principle of maximum entropy),
with a synchronic norm called \textsc{maxent} and a diachronic norm
called \emph{Infomin} which are consistent with each other.

What is at stake is whether information provides the right kind of
currency to address updating problems. While this position had a
strong advocate in E.T. Jaynes and is still prevalent in statistical
physics, almost all formal epistemologists, including Bayesians,
reject the notion that we can use information and its entropy to give
us objective updating methods. While this article addresses a specific
criticism of the \textsc{pme} and does not address this question in a
systematic manner, it seeks to contribute to a revival of interest in
information as currency for epistemological problems and to redefine
objectivism in probability updating (weakening it where it needs to be
weakened) so that it can regain some of its former respectability
among formal epistemologists.

While the \textsc{pme} initially attracted attention and was confirmed
especially by the work of Shore and Johnston (the \textsc{pme} uniquely
solves probability update problems provided one signs on to relatively
intuitive axioms, see \scite{7}{shorejohnson80}{}), there were vexing
counterexamples. Although there is formal proof that the \textsc{pme}
generalizes standard conditioning and Jeffrey conditioning, there are
problems where standard conditioning and Jeffrey conditioning do not
apply. The consensus emerged in the 1980s that the \textsc{pme} was a
helpful algorithm for these problems, but not a generally valid rule
of probability update. There are dissenting voices to this day, mostly
among physicists, but the consensus has been strong enough to be
incorporated in important textbooks.

\begin{itemize}
\item Brian Skyrms states, \qeins{\textsc{maxent} is not a generally
    valid updating rule} \scite{2}{skyrms87updating}{237}, based
  primarily on the counterexample provided by Abner Shimony (for
  example in \scite{7}{friedmanshimony71}{},
  \scite{7}{diasshimony81}{}, and \scite{7}{shimony85}{}). Skyrms
  makes this view known in several articles (see also
  \scite{7}{skyrms85}{} and \scite{7}{skyrms87dynamic}{}). In his
  textbook \emph{The Dynamics of Rational Deliberation}
  (\scite{10}{skyrms90}{}), there is a section on probability
  kinematics, but no reference to the \textsc{pme}.
\item Joseph Halpern argues in his textbook \emph{Reasoning About
    Uncertainty} (\scite{10}{halpern03}{}) that the \textsc{pme} is a
  promising candidate which delivers unique updated probability
  distributions, but there is counterintuitive behaviour in one
  specific case, the \emph{Judy Benjamin} case (see \scite{8}{halpern03}{110,
    119}). Therefore, the \textsc{pme} is a valuable tool that should
  be used with care and not be applied across the board (see
  \scite{8}{grovehalpern97}{110}).
\item In his textbook about ranking functions, called \emph{The Laws
    of Belief} (\scite{10}{spohn12}{}), Wolfgang Spohn admiringly
  refers to Wagner's \qeins{generalization of Jeffrey
    conditionalization} \scite{2}{spohn12}{41} and shows how well it
  accords with his own conditions for the dynamics of belief in terms
  of ranking functions (see \scite{8}{spohn12}{196ff}). Spohn cites
  Jeffrey's \emph{Linguist} problem at length and points out that
  the \textsc{pme}, another generalization of Jeffrey conditioning, is
  not compatible with Wagner's (and, we are led to conclude, also not
  compatible with Spohn's theory of ranking functions).
\end{itemize}

We will consider the above three counterexamples to be the main
arguments against the \textsc{pme} in practice (there are also more
theoretical objections, such as Skyrms' claim that the \textsc{pme} is
not an inductive rule but rather a rule for supposing; or the widely
shared non-Bayesian view, for example by Spohn and Halpern, that
numerical probabilities are even formally not always the most helpful
way to represent beliefs; or incompatibilities between epistemic
entrenchment and the \textsc{pme}---all of these and more would need to
be addressed for a systematic defence of the \textsc{pme} against its
competitors). Here is a fourth example from a textbook, however, which
underlines the influence that the three counterexamples above have had
in the literature.

\begin{itemize}
\item Without citing or engaging any one of the counterexamples, David
  MacKay writes in his textbook \emph{Information Theory, Inference,
    and Learning Algorithms} (\scite{10}{mackay03}{}), \qeins{maximum
    entropy is also sometimes proposed as a method for solving
    inference problems [\ldots] I think it is a bad idea to use
    maximum entropy in this way; it can give very silly answers}
  \scite{2}{mackay03}{308}.
\end{itemize}

Against the tide of scholarly consensus, I maintain that
the \textsc{pme} is defensible across all counterexamples and survives
as a unifying normative principle in probability update based on a
very simple intuition: that one should not illegitimately gain
information where the evidence does not provide it, and one should not
refuse to incorporate available information in updated beliefs. In
this paper, I have set myself the task of responding to Wagner's
\emph{Linguist} counterexample.\tbd{Insert Chomsky analogy from
  Sudelbuch p. 909}

To set the stage, I need to put a disclaimer on the modifier
\qnull{objective} in objective updating. I do not mean that every
rational agent should arrive at the same credences given the same
evidence. I also do not mean that rational agents should have one a
priori probability distribution which then becomes empirically
informed by observations so that her credences change purely by
application of objective updating methods (such as standard
conditioning) and the agent never really changes her mind. None of
these views are tenable and have become discredited as, let us call
it, Laplacian idealism.

What I am defending is not objectivism, but the logical element of
probability theory which John Maynard Keynes first proposed and which
among Bayesians has been largely replaced by subjectivism of one sort
or another. The logical element provides rules about which probability
distributions are acceptable to rational agents and how to proceed
from one probability distribution to another, given certain kinds of
evidence (other kinds of evidence may have to be dealt with in other
ways). It does not claim that everybody should arrive at the same
distribution (in as much as they claim to be rational), partly because
there is no initial point at which two rational agents must agree.
Just as is the case in deductive logic, we may come to a tentative and
voluntary agreement on a set of rules and presuppositions and then go
part of the way together.

I would call this view Laplacian realism and characterize it as
follows: 

\begin{enumerate}[(L1)]
\item There are logical rules which govern the probability
  distributions and the probability updates of rational agents which
  are intimately connected to information theory (Kolmogorov's axioms,
  standard conditioning, Jeffrey conditioning, \textsc{maxent},
  \emph{Infomin}).
\item Rational agents have some latitude in how they interpret
  evidence (especially with respect to assessing the independence
  relation of variables, for example when distinguishing between using
  synchronic or diachronic norms or when assessing epistemic
  entrenchments)---once the evidence is interpreted to yield formal
  constraints, however, the rational agent updates according to the
  logical rules outlined above, using all the available information
  without inappropriately using information that is not available
  (i.e.\ the rational agent keeps her distribution at maximum entropy
  given the available information)
\item Once an event space is specified, a rational agent assigns
  determinate probabilities to the events under consideration.
\end{enumerate}

The last condition is what we will use to show that the \textsc{pme},
rather than delivering the wrong results given Wagner's intuitions,
elegantly generalizes Jeffrey conditioning and Wagner conditioning to
boot. (L3), of course, is not uncontroversial. It corresponds to
Bayesians' traditional commitment to prior probabilities, but there
are many Bayesians now who advocate Bayesianism with a more human face
(Jeffrey's expression) and contend that even rational agents typically
lack determinate prior subjective probabilities and that their
opinions are characterized by imprecise credal states in response to
unspecific and equivocal evidence.

While I appreciate the equivocality of evidence, I would separate the
disambiguation of the evidence in articulating formal constraints from
bringing to bear a helpful formalism to probability update which
requires numerically precise priors. When we apply mathematics to
daily life, we do this all the time by measuring imprecisely and then
processing the disambiguated measurements using calculus. One
particularly strong advocate of imprecise credal states is James Joyce
(see \scite{8}{joyce05}{156f}), with the unfortunate consequence that
the updating strategies that Joyce proposes for these credal states
are impotent. No amount of evidence can modify the imprecise credal
state, because each member of the set of credal states that an agent
accepts has a successor (with respect to updating) that is also a
member of these credal states and that is consistent with its
predecessor and the evidence. Although the feeling is that the
imprecise credal state is narrowed by evidence towards more precision,
set theory clearly indicates that the credal state remains static, no
matter what the evidence is.\tbd{Maybe put this formally in an
  appendix.}

We could pursue this in more detail, but in the service of returning
to the natural generalization of Jeffrey conditioning, let me make
clear that Laplacian realism, as opposed to Laplacian idealism, is
entirely comfortable with retrospection (prior probabilities are only
fixed after the relevant event space has been identified, so that for
example Newton did not need to have a prior probability for Einstein's
theory in order to have a posterior probability for his theory of
gravity); and with subjectivity in selecting prior probabilities when
constraints both synchronically and diachronically have not yet been
meaningfully established (so that we escape the paradoxes of Bertrand
and von Mises, at the expense of objectivism about credences, rather
than updating under formally estabished constraints, which would
suspiciously recall Laplace's Demon in any case). Laplacian realism,
however, makes the traditional assumption that prior probabilities are
determinate, and not imprecise, once the event space is specified.

When these assumptions are granted, Wagner's attack against
the \textsc{pme} is revealed to be an attack against advocates of
the \textsc{pme} who violate the assumptions rather than the intuition
that Wagner fields against the \textsc{pme}. Because advocates of
the \textsc{pme} tend to be at least Laplacian realists (if not
trending towards Laplacian idealism), Wagner's point is misplaced and,
like Joyce, he should really be arguing for imprecise and
indeterminate priors. The intuition that Wagner mentions with respect
to the \emph{Linguist} counterexample is one that almost everybody
will share, but nobody has to violate it just because they accept
the \textsc{pme}, as Wagner asserts.
** Old Math I
According to Wagner (thus the index $w$ in $P_{w}$), the prior
probabilities for \emph{Linguist} are as follows (we will refer to
this table later on as table $\clubsuit$):

[shouldn't the Wagner constraint zeroes already be in here? -- big
problem: $\varrho$ is not justifiable]

\medskip

\begin{tabular}{|l|r|r|r|r|r|}\hline
$P_{w}$ & $\theta_{1}$ & $\theta_{2}$ & $\theta_{3}$ & $\theta_{4}$ & $P_{\Omega}$ \\ \hline
$\omega_{1}$ & ? & ? & ? & ? & $0.4\varrho$ \\ \hline 
$\omega_{2}$ & ? & ? & ? & ? & $0.3\varrho$ \\ \hline 
$\omega_{3}$ & ? & ? & ? & ? & $0.2\varrho$ \\ \hline 
$\omega_{4}$ & ? & ? & ? & ? & $0.1\varrho$ \\ \hline 
$\omega_{5}$ & ? & ? & ? & ? & $1-\varrho$ \\ \hline 
$P_{\Theta}$ & 0.20 & 0.30 & 0.40 & 0.10 & 1.00 \\ \hline
\end{tabular}
% \begin{tabular}{|l|r|r|r|r|r|r|}\hline
% $P_{w}$ & $\omega_{1}$ & $\omega_{2}$ & $\omega_{3}$ & $\omega_{4}$ & $\omega_{x}$ & $P_{\Theta}$ \\ \hline
% $\theta_{1}$ & ? & ? & ? & ? & ? & 0.20 \\ \hline
% $\theta_{2}$ & ? & ? & ? & ? & ? & 0.30 \\ \hline
% $\theta_{3}$ & ? & ? & ? & ? & ? & 0.40 \\ \hline
% $\theta_{4}$ & ? & ? & ? & ? & ? & 0.10 \\ \hline
% $P_{\Omega}$ & $0.4p_{x}$ & $0.3p_{x}$ & $0.2p_{x}$ & $0.1p_{x}$ & $1-p_{x}$ & 1.00\\ \hline
% \end{tabular}

\medskip

The cells in the middle of the table represent the joint probability
function on $\Omega\times\Theta$. $\omega_{5}$ is the negation of the
disjunction of $\omega_{i}$, $i=1,\ldots,4$, as in advance we do not
know what the native is going to say. We are assuming that
$\varrho=1-P_{\Omega}(\omega_{5})\neq{}0$, as logically possible
events should have non-zero probabilities, minute as they may be. In
Wagner's diction, we are not possessed of the joint probability
measure on $\omega\times\theta$ (thus the question marks), only of the
marginal probabilities. Applied to his later interpretation of what
(M) would say about the \emph{Linguist} problem, this
means that the advocate of (M) subject to Wagner's
criticism violates (L3).

Wagner's posterior probabilities $Q_{w}$ (in contrast to
(M)'s posterior probabilities $Q_{m}$ later on) for
\emph{Linguist} are as follows:

\medskip

\begin{tabular}{|l|r|r|r|r|r|}\hline
$Q_{w}$ & $\theta_{1}$ & $\theta_{2}$ & $\theta_{3}$ & $\theta_{4}$ & $Q_{\Omega}$ \\ \hline
$\omega_{1}$ & ? & ? & 0.00 & 0.00 & $0.40$ \\ \hline 
$\omega_{2}$ & ? & ? & 0.00 & 0.00 & $0.30$ \\ \hline 
$\omega_{3}$ & 0.00 & ? & 0.00 & ? & $0.20$ \\ \hline 
$\omega_{4}$ & ? & ? & 0.04 & ? & $0.10$ \\ \hline 
$\omega_{5}$ & 0.00 & 0.00 & 0.00 & 0.00 & $0.00$ \\ \hline 
$Q_{\Theta}$ & 0.30 & 0.60 & 0.04 & 0.06 & 1.00 \\ \hline
\end{tabular}

% \begin{tabular}{|l|r|r|r|r|r|r|}\hline
% $Q_{w}$ & $\omega_{1}$ & $\omega_{2}$ & $\omega_{3}$ & $\omega_{4}$ & $\omega_{x}$ & $Q_{\Theta}$ \\ \hline
% $\theta_{1}$ & ? & ? & 0.00 & ? & 0.00 & 0.30 \\ \hline
% $\theta_{2}$ & ? & ? & ? & ? & 0.00 & 0.60 \\ \hline
% $\theta_{3}$ & 0.00 & 0.00 & 0.00 & 0.04 & 0.00 & 0.04 \\ \hline
% $\theta_{4}$ & 0.00 & 0.00 & ? & ? & 0.00 & 0.06 \\ \hline
% $Q_{\Omega}$ & $0.4$ & $0.3$ & $0.2$ & $0.1$ & $0.00$ & 1.00\\ \hline
% \end{tabular}

% \begin{tabular}{|l|r|r|r|r|r|}\hline
%   $Q_{w}$ & $\omega_{1}$ & $\omega_{2}$ & $\omega_{3}$ & $\omega_{4}$ & $Q_{\Theta}$ \\ \hline
% $\theta_{1}$ & ? & ? & 0.00 & ? & 0.30 \\ \hline
% $\theta_{2}$ & ? & ? & ? & ? & 0.60 \\ \hline
% $\theta_{3}$ & 0.00 & 0.00 & 0.00 & 0.04 & 0.04 \\ \hline
% $\theta_{4}$ & 0.00 & 0.00 & ? & ? & 0.06 \\ \hline
% $Q_{\Omega}$ & 0.40 & 0.30 & 0.20 & 0.10 & 1.00\\ \hline
% \end{tabular}

\medskip
** Old Math II
Let $\omega_{j},j=1,\ldots,m$, and $\theta_{i},i=1,\ldots,n$, be
finite partitions of the event space with the joint prior probability
matrix $(p_{ij})$. Some of the elements of $(p_{ij})$ will be zero in
accordance with (\ref{eq:entail}), but never a whole row or a whole
column. Let $K_{\omega}\subseteq{}\{1,\ldots,m\}$ and
$K_{\theta}\subseteq{}\{1,\ldots,n\}$ be such that $p_{ij}=0$ for
$(i,j)\in{}K_{\omega}\times{}K_{\theta}$. We are now looking for the
matrix $(p_{ij})$ which obeys the following constraints and of all
such matrices has the highest Shannon entropy, if the elements of the
matrix are interpreted as a probability distribution:

\begin{equation}
  \label{eq:ce1}
\mbox{For all }i=1,\ldots,m\mbox{ it is true that }\sum_{j=1}^{n}p_{ij}=P(\omega_{i})
\end{equation}

\begin{equation}
  \label{eq:ce2}
\mbox{For all }j=1,\ldots,n\mbox{ it is true that }\sum_{i=1}^{m}p_{ij}=P(\theta_{j})
\end{equation}

\begin{equation}
  \label{eq:ce3}
\mbox{For all }(i,j)\in{}K_{\omega}\times{}K_{\theta}\mbox{ it is true that }p_{ij}=0
\end{equation}

$P(\theta_{j})$ is known for all $j=1,\ldots,n$. $P(\omega_{i})$ is
known for all $j=i,\ldots,m$ as dependent on $\varrho$ (see
table $\clubsuit$).

Advocates of (M), who are objectivist Bayesians, think
that determining these probabilities is not solely a matter of
consistency with probability axioms. If the marginal probabilities are
available, then the joint distribution is equal to the product of the
marginal distributions

\begin{equation}
  \label{eq:prod}
P(\theta_{i},\omega_{j})=p(\theta_{i})p(\omega_{j})\mbox{ for all
}i=1,\ldots,n\mbox{ and }j=1,\ldots,m
\end{equation}

to achieve maximum entropy (for a proof, see exercise 12.4 in
\scite{8}{coverthomas06}{421}).

Therefore, the prior probability table for \emph{Linguist}, from a
Laplacian's perspective, looks as follows:

\medskip

\begin{tabular}{|l|r|r|r|r|r|}\hline
  $P_{m}$ & $\omega_{1}$ & $\omega_{2}$ & $\omega_{3}$ & $\omega_{4}$ & $Q_{\Theta}$ \\ \hline
$\theta_{1}$ & 0.08 & 0.06 & 0.04 & 0.02 & 0.20 \\ \hline
$\theta_{2}$ & 0.12 & 0.09 & 0.06 & 0.03 & 0.30 \\ \hline
$\theta_{3}$ & 0.16 & 0.12 & 0.08 & 0.04 & 0.40 \\ \hline
$\theta_{4}$ & 0.04 & 0.03 & 0.02 & 0.01 & 0.10 \\ \hline
$Q_{\Omega}$ & 0.40 & 0.30 & 0.20 & 0.10 & 1.00\\ \hline
\end{tabular}

\medskip

Given the constraints (\ref{eq:entail}) and (\ref{eq:marg}) and a
formal result provided in the next section, (M)
solution minimizing the cross-entropy to this prior probability
distribution is

\medskip

\begin{tabular}{|l|r|r|r|r|r|}\hline
  $Q_{m}$ & $\omega_{1}$ & $\omega_{2}$ & $\omega_{3}$ & $\omega_{4}$ & $Q_{\Theta}$ \\ \hline
$\theta_{1}$ & 0.16 & 0.12 & 0.00 & 0.02 & 0.30 \\ \hline
$\theta_{2}$ & 0.24 & 0.18 & 0.15 & 0.03 & 0.60 \\ \hline
$\theta_{3}$ & 0.00 & 0.00 & 0.00 & 0.04 & 0.04 \\ \hline
$\theta_{4}$ & 0.00 & 0.00 & 0.05 & 0.01 & 0.06 \\ \hline
$Q_{\Omega}$ & 0.40 & 0.30 & 0.20 & 0.10 & 1.00\\ \hline
\end{tabular}

\medskip
** Old Math III
(M) delivers a reasonable solution conforming to the ad hoc intuitive
condition (W) imposed by Wagner. The solution that Wagner foists on
(M) is simply a victim of coarsening at random (see
\scite{7}{gruenwaldhalpern03}{}, who show how coarsening at random is
responsible for misguided intuitions in the \emph{Monty Hall} and the
\emph{Three Prisoners} problem). Just as the misguided interpretation
of the \emph{Monty Hall} problem operates on a coarsening of the event
space, Wagner's (M) solution operates on a coarsening of the event
space, which then fails to process the totality of the information
provided in the wording of the problem. Wagner's (M) solution uses a
probability distribution that is unnecessarily coarse---a more finely
grained prior probability distribution delivers the right results.

The calculations for \emph{Linguist} are awkward, because the
$4\times{}4$ joint probability matrix is too large to deal with on the
back of a napkin (and handling constraints on upper and lower
probabilities using Wagner's faulty assumptions is much more
complicated than handling constraints on joint probability
distributions as shown in the next section). Using my simplified
linguist problem above (where we only need to attend to a $2\times{}2$
matrix) makes the point just as beautifully, and again the
(M) solution concurs both with Wagner's condition and with
our intuitions: the posterior probability that the native is a
Protestant is 80\% versus a 20\% probability that he or she is a
Catholic. 

Here are the relevant tables for the simplified linguist problem
($\theta'_{1}$ means that the native is Catholic; $\theta'_{2}$ that
the native is Protestant; $\omega'_{1}$ means that the native's
utterance excludes the possibility that the native is Catholic;
$\omega'_{2}$ means that the native's utterance provides no
information about the native's religious affiliation). As before,
$P_{w}$ is Wagner's prior probability distribution (with lacunae),
$Q_{w}$ his posterior probability distribution. $Q_{w/m}$ is Wagner's
(faulty) interpretation of (M), based on $P_{w}$. $P_{m}$
and $Q_{m}$ are the correct versions of applying (M) to
the simplified linguist problem. The prior probability distributions
are prior in the sense that they are prior to the information about
what the two utterances entail.

\medskip

\begin{tabular}{|l|r|r|r|c|l|r|r|r|}\hline
$P_{w}$ & $\omega'_{1}$ & $\omega'_{2}$ & $P_{\Theta'}$ & \cellcolor[gray]{0.9} & $Q_{w}$ & $\omega'_{1}$ & $\omega'_{2}$ & $Q_{\Theta'}$ \\ \hline
$\theta'_{1}$ & ? & ? & 0.50 & \cellcolor[gray]{0.9} & $\theta'_{1}$ & 0.60 & 0.20 & 0.80 \\ \hline
$\theta'_{2}$ & ? & ? & 0.50 & \cellcolor[gray]{0.9} & $\theta'_{2}$ & 0.00 & 0.20 & 0.20 \\ \hline
$P_{\Omega'}$ & 0.60 & 0.40 & 1.00 & \cellcolor[gray]{0.9} & $Q_{\Omega'}$ & 0.60 & 0.40 & 1.00\\ \hline
\cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} \\ \hline
\cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & $Q_{w/m}$ & $\omega'_{1}$ & $\omega'_{2}$ & $Q_{\Theta'}$ \\ \hline
\cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & $\theta'_{1}$ & 0.60 & 0.00 & 0.60 \\ \hline
\cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & $\theta'_{2}$ & 0.00 & 0.40 & 0.40 \\ \hline
\cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & $Q_{\Omega'}$ & 0.60 & 0.40 & 1.00\\ \hline
\cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} & \cellcolor[gray]{0.9} \\ \hline
$P_{m}$ & $\omega'_{1}$ & $\omega'_{2}$ & $P_{\Theta'}$ & \cellcolor[gray]{0.9} & $Q_{m}$ & $\omega'_{1}$ & $\omega'_{2}$ & $Q_{\Theta'}$ \\ \hline
$\theta'_{1}$ & 0.30 & 0.20 & 0.50 & \cellcolor[gray]{0.9} & $\theta'_{1}$ & 0.60 & 0.20 & 0.80 \\ \hline
$\theta'_{2}$ & 0.30 & 0.20 & 0.50 & \cellcolor[gray]{0.9} & $\theta'_{2}$ & 0.00 & 0.20 & 0.20 \\ \hline
$P_{\Omega'}$ & 0.60 & 0.40 & 1.00& \cellcolor[gray]{0.9} & $Q_{\Omega'}$ & 0.60 & 0.40 & 1.00\\ \hline
\end{tabular}

\medskip

As in \emph{Linguist}, $Q_{w}$ and $Q_{m}$ agree on the solution of
the simplified linguist problem where $Q_{w}$ has specified
probabilities, especially in the margins where all of $Q_{w}$'s
probabilities are specified in both problems.
** Old Abstract
  This article demonstrates that Wagner's application of the principle
  of maximum entropy is incorrect and that a correct application
  agrees with his intuition. It presents a formal proof that the
  principle of maximum entropy seamlessly and elegantly generalizes
  not only standard conditioning and Jeffrey conditioning (as is
  well-documented in the literature) but also Wagner's generalization.
** Last Section (SC/JC/WC)
\section{The PME is the Natural Generalization of Jeffrey Conditioning}
\label{maxjeff}

We will show, using Lagrange multipliers, that (M)
conforms to the intuition voiced by Wagner that the prior probability
distribution remains operative in the assessment of relative
uncertainties with respect to the consequents of observed
conditionals. Wagner denies that (M) conforms to this
intuition, thus disparaging (M), but we have shown in the
previous section that Wagner's judgment relies on an interpretation of
(M) in terms of non-Bayesian assumptions, which is
nonsensical as (M) is a more specific version of
Baysianism.

We will show that (M) conforms to the intuitions behind
standard conditioning; then that it conforms to the intuitions behind
Jeffrey conditioning; and finally that it conforms to Wagner's ad hoc
\qnull{natural generalization of Jeffrey conditioning.} Throughout, to
make formalities comprehensible for non-mathematical folk, we will
refer to finite (and therefore discrete) probability distributions.
For countable and continuous probability distributions, the reasoning
is largely analogous (for a mathematically rigorous introduction to
continuous entropy see \scite{8}{guiasu77}{16ff}; for an example of
how to do a proof of this section for continuous probability densities
see \scite{8}{catichagiffin06}{11}; for a proof that the stationary
points of the Lagrange function are indeed the desired extrema see
\scite{8}{zubarev96}{55} and \scite{8}{coverthomas06}{410}; for the
pioneer of the method applied in this section see
\scite{8}{jaynes79}{241ff}).

\medskip

{\noindent}\emph{Standard Conditioning}

\medskip

{\noindent}Let $y_{i}$ (all $y_{i}\neq{}0$) be a finite prior
probability distribution summing to $1$, $i\in{}I$. Let $x_{i}$ be the
posterior probability distribution derived from standard conditioning
with $x_{i}=0$ for all $i\in{}I'$ and $x_{i}\neq{}0$ for all
$i\in{}I''$, $I'\cup{}I''=I$. $I'$ and $I''$ specify the standard
event observation. Standard conditioning requires that

\begin{equation}
  \label{eq:sc}
  x_{i}=\frac{y_{i}}{\sum_{k\in{}I''}y_{k}}.
\end{equation}

To solve this problem using (M), we want to minimize the
cross-entropy with the constraint that the non-zero $x_{i}$ sum to
$1$. The Lagrange function is (writing in vector form
$x=(x_{i})_{i\in{}I''}$)

\begin{equation}
  \label{eq:sclag}
  \Lambda(x,\lambda)=\sum_{i\in{}I''}x_{i}\ln\frac{x_{i}}{y_{i}}+\lambda\left(1-\sum_{i\in{}I''}x_{i}\right).
\end{equation}

Differentiating the Lagrange function with respect to $x_{i}$ and
setting the result to zero gives us

\begin{equation}
  \label{eq:sc1}
  x_{i}=y_{i}e^{\lambda-1}
\end{equation}

with $\lambda$ normalized to

\begin{equation}
  \label{eq:sc2}
  \lambda=-1+\ln{}\sum_{i\in{}I''}y_{i}.
\end{equation}

(\ref{eq:sc}) follows immediately. (M) and standard
conditioning are consistent with each other.

\medskip

{\noindent}\emph{Jeffrey Conditioning}

\medskip

{\noindent}Let $\theta_{i},i=1,\ldots,n$ and
$\omega_{j},j=1,\ldots,m$ be finite partitions of the event space with
the joint prior probability matrix $(y_{ij})$ (all $y_{ij}\neq{}0$).
Let $x_{ij}$ be the posterior probability distribution derived from
Jeffrey conditioning with 

\begin{equation}
  \label{eq:jc1}
  \sum_{i=1}^{n}x_{ij}=\alpha_{j}\mbox{ for all }j=1,\ldots,m
\end{equation}

where the $\alpha_{j}\neq{}0, \sum\alpha_{j}=1$ are the observed
redistribution of the marginal probability for $\omega_{j}$. Jeffrey
conditioning requires that for all $i=1,\ldots,n$

\begin{equation}
  \label{eq:jc2}
  q(\theta_{i})=\sum_{j=1}^{m}p(\theta_{i}|\omega_{j})q(\omega_{j})=\sum_{j=1}^{m}\frac{y_{ij}}{p(\omega_{j})}q(\omega_{j})
\end{equation}

where $p$ and $q$ are the marginal probabilities for
$(\theta_{i})_{i=1,\ldots,n}$ and $(\omega_{j})_{j=1,\ldots,m}$, prior
and posterior respectively (just as $P$ and $Q$ are the joint
probabilities on $\Theta\times\Omega$), for example
$\alpha_{j}=q(\omega_{j})$, $P(\theta_{i},\omega_{j})=y_{ij}$ and

\begin{equation}
  \label{eq:jc3}
  q(\theta_{i})=\sum_{j=1}^{m}x_{ij}.
\end{equation}

Using (M) to get the posterior distribution $(x_{ij})$,
the Lagrange function is (writing in vector form
$\lambda=(\lambda_{1},\ldots,\lambda_{m})$ and
$x=(x_{11},\ldots,x_{n1},\ldots,x_{nm})$)

\begin{equation}
  \label{eq:jclag}
  \Lambda(x,\lambda)=\sum_{i=1}^{n}\sum_{j=1}^{m}x_{ij}\ln\frac{x_{ij}}{y_{ij}}+\sum_{j=1}^{m}\lambda_{j}\left(\alpha_{j}-\sum_{i=1}^{n}x_{ij}\right).
\end{equation}

Consequently,

\begin{equation}
  \label{eq:jc4}
  x_{ij}=y_{ij}e^{\lambda_{j}-1}
\end{equation}

with the Lagrangian parameters $\lambda_{j}$ normalized by

\begin{equation}
  \label{eq:jc5}
  \sum_{i=1}^{n}y_{ij}e^{\lambda_{j}-1}=\alpha_{j}
\end{equation}

(\ref{eq:jc2}) follows immediately. (M) and Jeffrey
conditioning are consistent with each other.

\medskip

{\noindent}\emph{Wagner Conditioning}

\medskip

{\noindent}Let $\theta_{i},i=1,\ldots,n$ and
$\omega_{j},j=1,\ldots,m$ be finite partitions of the event space. Let
the joint prior probability matrix be $(y_{ij})$ and the joint
posterior probability matrix be $(x_{ij})$. The elements of this
matrix may be unknown or may have to be inferred from one's state of
ignorance or uncertainty or may be chosen any other way according to
basic probability axioms and your favourite interpretation of
probability. The marginal probabilities are
$p(\theta_{i}),p(\omega_{j}),q(\theta_{i}),q(\omega_{j})$ so that, for
example,

\begin{equation}
  \label{eq:wc1}
  p(\theta_{i})=\sum_{j=1}^{m}y_{ij}
\end{equation}

if the pertinent $y_{ij}$ exist. According to (\ref{eq:qofa}), Wagner
conditioning determines the posterior marginal probability to be

\begin{equation}
  \label{eq:wc2}
    q(\theta_{i})=\sum_{E\in\mathcal{E}}m(E)p(\theta_{i}|E)\mbox{ for
      all }i=1,\ldots,n
\end{equation}

given the conditions in (\ref{eq:entail}) and (\ref{eq:marg}).
(\ref{eq:entail}) means for $Q$ that depending on the observed
conditionals there is a partition of the indices
$\{1,\ldots,n\}\times\{1,\ldots,m\}$ into sets $K'$ and $K''$ such
that $Q(\theta_{i},\omega_{j})=0$ (i.e.\ $x_{ij}=0$) for
$(i,j)\in{}K'$ and $Q(\theta_{i},\omega_{j})\neq{}0$ (i.e.\
$x_{ij}\neq{}0$) for $(i,j)\in{}K''$.

We want to show that (M) comes to the same conclusion as
Wagner conditioning, provided that the probability matrices
$(y_{ij}),(x_{ij})$ are fully quantified---to which we commit
ourselves since (M) only makes sense in a Bayesian
framework. Because the matrices are fully quantified, we rewrite
(\ref{eq:wc2}) as

\begin{equation}
  \label{eq:wc3}
  q(\theta_{i})=p(\theta_{i})\sum_{j=1}^{m}\frac{\sum_{k=1}^{n}y_{kj}}{\sum_{r=1}^{n\circledast{}j}p(\theta_{r})}
\end{equation}

where for $j=1,\ldots,m$

\begin{equation}
  \label{eq:dast}
  \sum_{r=1}^{n\circledast{}j}p(\theta_{r})=\sum_{r=1,(r,j)\in{}K''}^{n}p(\theta_{r})
\end{equation}

Note that for all $i,k=1,\ldots,n$ and $j=1,\ldots,m$

\begin{equation}
  \label{eq:wc4}
  \frac{y_{ij}}{y_{kj}}=\frac{p(\theta_{i})}{p(\theta_{k})}
\end{equation}

because of (\ref{eq:prod}). The Lagrange function is, given the
above-mentioned constraints,

\begin{equation}
  \label{eq:wclag}
  \Lambda(x,\lambda)=\sum_{i=1}^{n}\sum_{j=1}^{m\circledast{}i}x_{ij}\ln{}x_{ij}+\sum_{j=1}^{m}\lambda_{j}\left(\sum_{i=1}^{n}y_{ij}-\sum_{i=1}^{n\circledast{}j}x_{ij}\right)
\end{equation}

Differentiating $\Lambda$ with respect to $x_{ij}$ gives us for $(i,j)\in{}K''$

\begin{equation}
  \label{eq:wc5}
  x_{ij}=y_{ij}e^{\lambda_{j}-1}
\end{equation}

with the Lagrangian parameters $\lambda_{j}$ normalized such that

\begin{equation}
  \label{eq:wc6}
  \mu_{j}=e^{\lambda_{j}-1}=\frac{\sum_{i=1}^{n}y_{ij}}{\sum_{i=1}^{n\circledast{}j}y_{ij}}.
\end{equation}

The $\mu_{j}$ provide a simple (and correct) way to arrive at
(M) solutions for these types of problems, compared to the
complicated (and incorrect) way in which Wagner arrives at
(M) solutions. Comparing the correct (M)
solution gained with the help of Lagrangian multipliers

\begin{equation}
  \label{eq:wc7}
  q(\theta_{i})=\sum_{j=1}^{m}\mu_{j}y_{ij}
\end{equation}

to Wagner's solution in (\ref{eq:wc3}) it is clear that they agree if
and only if

\begin{equation}
  \label{eq:wc8}
  \frac{y_{ij}}{y_{kj}}=\frac{p(\theta_{i})}{p(\theta_{k})}
\end{equation}

which is (\ref{eq:wc4}). (M) and Wagner conditioning are
consistent with each other.

This could not have been any easier, given that the proofs with
respect to standard conditioning and Jeffrey conditioning are readily
available in the literature. (M), when it is not
adulterated by viewing probabilities as properties of the external
world rather than representations of uncertainty in the agents who
entertain them, seamlessly and elegantly handles the observation of
conditionals.
** A simple problem
A simple problem. A simple solution. Counterexamples. Innocence lost.
We may be telling the story of knowledge and its properties, of
Plato's JTB, Gettier cases, innocence lost in the decades following
Gettier, and perhaps some innocence regained in Timothy Williamson's
knowledge first approach. Yet instead of addressing the most
interesting problem in epistemology, we are addressing what we hold to
be the most interesting problem in formal epistemology: probability
update. 

Imagine a situation in which you hold a probability distribution that
obeys the fundamental axioms of probability theory with respect to a
set of outcomes. Then you make an observation, or you receive new
evidence. Logic and mathematics do not forbid that you change your
probabilities any way you like, provided that they still obey the
axioms. Formal epistemology seeks to introduce some normativity to
this process, based on assumptions as thin and as widely shared as
possible. As in so many areas of philosophy, we must find a reflective
equilibrium between the simplicity of laws and intuitions about cases.

Here is the simple solution to our simple problem of probability
update: use the posterior probability distribution which is in accord
with your observation or new evidence while gaining as little
information as possible. After all, why should you glean more
information than necessary from your evidence. This simple solution
has remarkable properties. It seamlessly generalizes standard
conditioning and Jeffrey's rule, two much less contested methods of
probability update used under more narrowly defined circumstances. If
I get you to assent to a few basic rationality assumptions (some, of
course, have questioned if you should assent to them, see Uffink),
then I can provide you with mathematical proof that the simple
solution will give you the unique acceptable solution to your updating
problem (see Shore and Johnston). As icing on the cake, the simple
solution is optimal under some plausible logarithmic scoring rule and
fulfills the entropy concentration phenomenon so that other solutions
are uniquely dense around it (no other solution has this property).

Ignore the icing on the cake. It is its simplicity that makes this
solution so compelling: gain as little information from your evidence
as possible. Much stupidity in the world comes from people drawing
conclusions to which they are not entitled by their evidence. This may
be an evolutionarily successful strategy to get somewhere with a short
and brutish life. The principle of maximum entropy, which is the name
of the simple solution outlined above, shows us a way to come to more
circumspect conclusions, given certain pragmatic assumptions (that we
have gotten hold of a prior probability distribution, for example,
about whose origin we will not worry here; that we want to be
reasonable in a certain way that we will not specify here). 

Then there are counterexamples. A famous counterexample is Bas van
Fraassen's story around the soldier Judy Benjamin who navigates
between enemy territory and home headquarters. She gains a certain
sense of security from the principle of maximum entropy that our
intuition will not allow her. Another famous counterexample is Abner
Shimony's Lagrangian parameter, about which an adherent of maximum
entropy must have absolute certainty without much evidence at all.
This is hardly acceptable for someone who prizes entropy (entropy is
the diffusion of information and is in some ways opposite to the lack
of entropy which we find in absolute certainty). 

Philosophers are very impressed with these counterexamples. Joseph
Halpern rejects the principle of maximum entropy as a simple solution
to our simple problem because of Judy Benjamin. Brian Skyrms rejects
the principle of maximum entropy as a simple solution to our simple
problem because of Shimony's parameter. And now that the simple
solution has been debunked philosophers are in heaven finding more
complicated solutions and asking more complicated questions.
Most physicists, however, tenaciously hold on to the principle of
maximum entropy, defending its status as a general solution to the
problem of probability update, for physicists use the principle of
maximum entropy all the time and are loath to worry about its
applicability and the complications in which philosophers tend to
revel. These are stereotypes of philosophers and physicists, but they
give me a stage to introduce the linguist.

The linguist is a character in a less well-known counterexample to the
principle of maximum entropy forwarded by Carl Wagner (see
\scite{7}{wagner92}{}). Just as is the case for Judy Benjamin and
Shimony's parameter, there is both simplicity and a powerful and
immediate appeal to intuition in this counterexample. And just as is
the case for the previous two counterexamples, I argue that our
intuitions are being manipulated by them and that a closer look
reveals the generality of the principle of maximum entropy, applying
to them just as validly and intuitively as across the board, although
one must undertake the effort of a closer look. Section one of this
paper introduces the Linguist, as we will call this case from now on,
as well as Wagner's solution, which is in violation of the principle
of maximum entropy. Section two provides the solution that the
principle of maximum entropy proposes for this problem. Section three
outlines the arguments why the latter solution is better than the
former. I refer the reader to the literature for the Judy Benjamin
case and Shimony's parameter case. For the Linguist, I hope that this
paper renders similar services in defence of maximum entropy.
Sometimes innocence is worth fighting for.
* quotes
** Maximum entropy joint distribution from marginals?
http://math.stackexchange.com/questions/496584/maximum-entropy-joint-distribution-from-marginals
* buffer
2000__V_Majernik__Marginal_Probability_Distribution_Determined_by_the_Maximum_Entropy_Method
